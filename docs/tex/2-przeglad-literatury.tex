\clearpage % Rozdziały zaczynamy od nowej strony.
\section{Przegląd literatury}

\subsection{Zastosowanie LLM w generowaniu kodu infrastruktury}

Podejście Infrastructure as Code polega na zarządzaniu infrastrukturą IT poprzez definiowanie jej konfiguracji w postaci czytelnego dla maszyny kodu. Zapewnia to automatyzację, spójność środowisk, kontrolę wersji oraz skalowalność zarządzania infrastrukturą. Tworzenie i utrzymanie takiej infrastruktury bywa jednak wymagające, stąd coraz większe zainteresowanie automatyzacją tych zadań z użyciem AI. Duże modele językowe, trenowane na ogromnych zbiorach danych tekstowych i kodu, wykazały zdolność do generowania i rozumienia kodu, co czyni je obiecującym narzędziem do automatycznego tworzenia konfiguracji IaC. W ostatnich latach pojawiło się wiele prac badających zastosowanie LLM do różnych platform i języków IaC. Srivatsa i in. (2024) \cite{srivatsa_survey_2024} przedstawili przekrojowe spojrzenie na ten temat w swojej pracy przeglądowej, podkreślając, że automatyzacja IaC za pomocą LLM może zredukować nakład pracy ludzkiej i błędy, jeśli uda się odpowiednio ukierunkować model. W pracy tej omówiono również wyzwania stojące przed takimi rozwiązaniami oraz perspektywy dalszych badań.

Aktualne badania integrujące LLM z narzędziami chmurowymi pokazują obiecujące rezultaty, choć są to dopiero początki. Lanciano i in. (2023) \cite{lanciano_analyzing_2023} zaproponowali wyspecjalizowany model analizujący pliki wdrożeniowe Kubernetes, aby wspomóc mniej doświadczonych użytkowników w projektowaniu konfiguracji i zapewnieniu ich jakości. Z kolei Xu i in. (2023) \cite{ueno_migrating_2024} wprowadzili benchmark CloudEval-YAML do oceny zdolności LLM w generowaniu konfiguracji aplikacji cloud-native (w formacie YAML), udostępniając zestaw testów jednostkowych do weryfikacji poprawności wygenerowanych manifestów. Inni badacze sugerują zastosowanie LLM również do automatycznego monitorowania i utrzymania systemów – np. Malul i in. (2024) \cite{malul_genkubesec_2024} zaproponowali pipeline wykorzystujący LLM do detekcji anomalii i autoremediacji w środowisku Kubernetes, w celu zwiększenia stabilności systemu. Podejścia te często bazują na trenowaniu dedykowanych modeli językowych dopasowanych do domeny konfiguracji (np. z użyciem specjalistycznych korpusów danych). Alternatywą dla kosztownego trenowania od podstaw jest wykorzystanie istniejących, ogólnych modeli (GPT-3.5, GPT-4, Llama2 itp.) i dostrajanie \cite{liu_pre-train_2023} ich za pomocą odpowiednich promptów (instrukcji dla modelu) \cite{kratzke_dont_2024}. Niniejsza praca wpisuje się w ten nurt – badano, jak standardowe modele językowe mogą być wykorzystane (bez dodatkowego trenowania) do generowania plików Dockerfile i manifestów Kubernetes zgodnych z wymaganiami DevOps, poprzez odpowiednie techniki inżynierii promptów.

Ważnym kierunkiem rozwoju są również benchmarki oceny jakości konfiguracji IaC generowanych przez modele językowe. Kon i in. zaprezentowali benchmark IaC-Eval, który służy do systematycznej oceny jakości konfiguracji dla usług chmurowych AWS \cite{kon_iac-eval_nodate}. Narzędzie to umożliwia bardziej obiektywną ocenę możliwości modeli językowych w kontekście generowania poprawnych, zgodnych z intencją użytkownika konfiguracji.

Pujar i in. (2023) \cite{pujar_invited_2023} zaproponowali natomiast wykorzystanie LLM do generowania konfiguracji IT w języku Ansible YAML, opracowując narzędzie Ansible Wisdom. Ich praca pokazuje, że techniki prompt engineering można z powodzeniem zastosować również do innych języków deklaratywnych, nie tylko Dockerfile czy manifestów Kubernetes.

Reasumując, liczne prace badawcze wykazały potencjał LLM w automatyzacji tworzenia konfiguracji IaC – zarówno poprzez integrację z narzędziami chmurowymi, jak i dostosowanie technik promptowania do specyfiki deklaratywnych języków konfiguracyjnych. Jednocześnie zauważono szereg wyzwań, takich jak błędne generacje (halucynacje), aspekty bezpieczeństwa czy zgodność z zamierzeniami użytkownika \cite{malul_genkubesec_2024, kratzke_dont_2024}.

\subsection{Generowanie konfiguracji Docker przy użyciu LLM}

Docker stał się standardem w dziedzinie konteneryzacji aplikacji, umożliwiając tworzenie przenośnych, izolowanych środowisk wykonawczych. Jednak tworzenie efektywnych i bezpiecznych plików Dockerfile wymaga specjalistycznej wiedzy i doświadczenia.

Blog Docker \cite{docker_genai} przedstawia innowacyjne podejście do generowania plików Dockerfile przy użyciu GenAI. Autorzy demonstrują, jak wyposażenie asystenta AI w narzędzia do analizy projektu pozwala na generowanie bardziej adekwatnych plików Dockerfile. Zamiast polegać wyłącznie na ogólnej wiedzy modelu, asystent może bezpośrednio analizować kod źródłowy projektu, określać jego typ (np. projekt NPM) i na tej podstawie generować odpowiedni Dockerfile. Co więcej, autorzy podkreślają znaczenie inkorporowania najlepszych praktyk do procesu generowania, co pozwala uniknąć typowych problemów, takich jak używanie przestarzałych obrazów bazowych czy pomijanie mechanizmów buforowania.

\begin{lstlisting}[caption={Dockerfile wygenerowany przez LLM},label={lst:example-dockerfile},captionpos=b,numbers=left]
# Stage 1 - Downloading dependencies
FROM node:22-slim AS deps
WORKDIR /usr/src/app
COPY package*.json ./
RUN --mount=type=cache,target=/root/.npm npm ci --omit=dev

# Stage 2 - Building application
FROM deps AS build
RUN --mount=type=cache,target=/root/.npm npm ci && npm build

# Stage 3 - Using a recommended base image from Scout
FROM node:22-slim
WORKDIR /usr/src/app
COPY --from=deps /usr/src/app/node_modules ./node_modules
COPY --from=build /usr/src/app/dist ./dist
CMD [ "npm", "start" ]    
\end{lstlisting}

Powyższy listing \ref{lst:example-dockerfile} wygenerowanego przez LLM pliku Dockerfile ilustruje, jak model może zastosować najlepsze praktyki, takie jak budowa wieloetapowa, używanie rekomendowanych obrazów bazowych oraz mechanizmów cache.

Jednym z pionierskich rozwiązań praktycznych integrujących LLM z DevOps jest projekt Repo2Run – agent oparty na LLM, zaprojektowany do automatycznego konfigurowania środowisk i generowania plików Dockerfile dla dowolnych repozytoriów Python. Narzędzie to zostało zaprezentowane przez Hu i in. (2025) \cite{hu_llm-based_2025} i stanowi próbę pełnej automatyzacji procesu tworzenia kontenera na podstawie kodu aplikacji. Problem, jaki adresuje Repo2Run, to złożoność ręcznego przygotowania Dockerfile dla istniejącego projektu – np. ustalenie zależności, właściwej bazy obrazu, komend instalacji pakietów – a także ryzyko błędów podczas konfiguracji środowiska (np. niepowodzenie jednej komendy może zostawić środowisko w nieokreślonym stanie). Repo2Run rozwiązuje to poprzez dwupoziomową architekturę środowiska uruchomieniowego. Model LLM działa wewnątrz odizolowanego kontenera (środowisko wewnętrzne) i wykonuje kolejne polecenia konfiguracyjne, podczas gdy otaczające go środowisko zewnętrzne monitoruje przebieg i w razie potrzeby modyfikuje kontekst (np. zmienia bazowy obraz Docker). Wprowadzono mechanizm rollback, który przy każdym błędzie cofa stan kontenera do poprzedniego stabilnego punktu, zapobiegając tzw. „zanieczyszczeniu” środowiska nieudanymi komendami. Równolegle działa komponent generujący Dockerfile – na podstawie zestawu wykonanych z sukcesem poleceń buduje on finalny plik Dockerfile, pomijając te instrukcje, które powodowały błędy, aby wynikowy obraz dawał się zbudować bez błędów. Według autorów, Repo2Run jest pierwszym podejściem wykorzystującym agenta LLM do automatycznego wygenerowania Dockerfile z istniejącego repozytorium kodu w pełni samodzielnie. Skuteczność rozwiązania oceniono na zbiorze 420 popularnych projektów Pythona z testami jednostkowymi – w 86\% przypadków Repo2Run zdołał poprawnie skonfigurować środowisko i wygenerować działający Dockerfile dla danego projektu. Tak wysoki odsetek pokazuje potencjał LLM w automatyzacji tworzenia kontenerów, choć warto zauważyć, że około 14\% przypadków nadal wymaga interwencji (co może wynikać z nietypowych zależności lub ograniczeń modelu w rozumieniu specyfiki danego projektu).

\subsection{Zastosowanie LLM w generowaniu i zarządzaniu konfiguracjami Kubernetes}

Kubernetes, jako standardowa platforma do orkiestracji kontenerów w środowiskach chmurowych, oferuje potężne możliwości w zakresie zarządzania aplikacjami kontenerowymi, ale jednocześnie charakteryzuje się wysokim poziomem złożoności. Tworzenie poprawnych manifestów Kubernetes – złożonych plików YAML opisujących obiekty takie jak wdrożenia, usługi czy konfiguracje – stanowi wyzwanie nawet dla doświadczonych specjalistów DevOps. Ze względu na deklaratywny charakter manifestów i ich szczegółowe wymagania, każdy błąd lub pominięcie może prowadzić do nieprawidłowego działania aplikacji. Poprawne przygotowanie konfiguracji wymaga dobrej znajomości API Kubernetes oraz najlepszych praktyk projektowych.

Kratzke i Drews (2024) \cite{kratzke_dont_2024} zaproponowali podejście “Don’t Train, Just Prompt”, w którym wykorzystali istniejące modele językowe (m.in. GPT-3.5, GPT-4 oraz otwarte modele Llama2, Mistral) do generowania manifestów Kubernetes na podstawie opisów tekstowych, bez trenowania modeli od zera. Zastosowali różne techniki inżynierii promptów, w tym tryb zero-shot (model generuje manifest tylko na podstawie pojedynczej instrukcji), few-shot (model otrzymuje kilka przykładów poprawnych manifestów jako kontekst) oraz prompt-chaining (łańcuch promptów, gdzie wynik pierwszego zapytania jest iteracyjnie korygowany w kolejnym).

Wyniki ich badań są zachęcające: nawet bez dostrajania, duże modele były w stanie wygenerować poprawne manifesty spełniające podane wymagania konfiguracyjne, choć czasem wymagały drobnych poprawek przez człowieka. W szczególności modele GPT-4 oraz GPT-3.5 okazały się na tyle potężne, że umożliwiły w pewnych przypadkach w pełni automatyczne wdrożenie aplikacji na Kubernetes – od specyfikacji słownej do działającego klastru – bez konieczności ręcznej ingerencji DevOps. Co ciekawe, autorzy zauważyli, że nie zawsze „większy znaczy lepszy” – mniejsze modele (jak np. Llama2 13B) przy odpowiednim przygotowaniu promptu potrafiły dorównać, a nawet przewyższyć dokładnością większe modele w generowaniu niektórych fragmentów manifestu. Kwestionuje to powszechne założenie, że tylko największe modele są użyteczne, wskazując jednocześnie na dużą rolę właściwej konfiguracji zapytania (promptu) w osiągnięciu dobrych wyników.

W podsumowaniu pracy autorzy podkreślają, że opracowanie skutecznych promptów jest kluczowe dla uzyskania wysokiej jakości konfiguracji z modelu językowego oraz że podejście to stanowi obiecujący kierunek rozwoju automatyzacji DevOps. Sugestie te potwierdzają, iż z odpowiednimi technikami LLM może znacząco ułatwić tworzenie manifestów, czyniąc zarządzanie Kubernetes bardziej intuicyjnym i mniej obarczonym ryzykiem błędu ludzkiego.

W praktyce migracji aplikacji do chmury istotny jest również scenariusz konwersji istniejących konfiguracji kontenerów do formatu Kubernetes. Ueno i Uchiumi (2024) \cite{ueno_migrating_2024} zaproponowali wykorzystanie LLM do automatycznej migracji z Docker Compose do Kubernetes. Docker Compose jest prostszym narzędziem do definiowania wielokontenerowych aplikacji, często używanym przez programistów lokalnie, podczas gdy Kubernetes wymaga bardziej szczegółowych manifestów do wdrożenia w środowisku produkcyjnym. Różnica poziomu abstrakcji między Compose a Kubernetes sprawia, że ręczna migracja bywa czasochłonna i podatna na pomyłki.

Autorzy opracowali benchmark pozwalający ocenić, jak dobrze model językowy (np. ChatGPT) radzi sobie z tłumaczeniem pliku docker-compose.yml na odpowiadające mu manifesty Kubernetes. Ocena obejmowała trzy kryteria: (1) czytelność i zrozumiałość wygenerowanego kodu dla programisty (np. czy zachowano przejrzystą strukturę i komentarze), (2) zgodność ze specyfikacją wejściową (czy manifest Kubernetes rzeczywiście odpowiada usługom/zależnościom z oryginalnego Compose) oraz (3) spójność i kompletność wygenerowanej konfiguracji.

Wyniki eksperymentów pokazały, że LLM potrafią zasadniczo poprawnie konwertować typowe pliki Compose – w wielu przypadkach dodając nawet brakujące informacje, które nie były jawnie podane, a są wymagane przez Kubernetes (np. tworząc domyślne definicje dla elementów, które Compose traktuje w uproszczony sposób). Innymi słowy, model uzupełniał „luki” w specyfikacji, dostarczając działający manifest Kubernetes nawet z niepełnych danych wejściowych.

Zauważono jednak dwa znaczące ograniczenia. Po pierwsze, brak komentarzy – wygenerowane manifesty zwykle nie zawierały żadnych adnotacji ani objaśnień, przez co traciły na czytelności z punktu widzenia dewelopera. Ręcznie pisane konfiguracje często zawierają komentarze wyjaśniające nietypowe ustawienia; model językowy pomijał tę warstwę, skupiając się wyłącznie na kodzie. Po drugie, przy nietypowych lub niejasnych specyfikacjach wejściowych jakość wyników znacząco spadała. Jeśli intencje użytkownika nie były jednoznaczne lub plik Compose zawierał niekonwencjonalne konstrukcje, LLM miewał trudności z poprawnym wygenerowaniem odpowiedników w Kubernetes.

Mimo tych wad, praca Ueno i Uchiumi potwierdza, że LLM mogą wspomóc proces migracji do chmury, automatyzując dużą część pracy konfiguracyjnej. Wskazuje jednocześnie na potrzebę dalszych usprawnień – np. integracji mechanizmów dodających objaśnienia do kodu, czy lepszego radzenia sobie z niejednoznacznymi przypadkami poprzez dopytywanie użytkownika lub bardziej zaawansowane prompty.

\begin{lstlisting}[caption={Manifest Kubernetes wygenerowany przez LLM},label={lst:example-k8s},captionpos=b,numbers=left]
apiVersion: apps/v1
kind: Deployment
metadata:
  name: example-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: example-app
  template:
    metadata:
      labels:
        app: example-app
    spec:
      containers:
      - name: example-app
        image: example-image:latest
        ports:
        - containerPort: 8080
        resources:
          limits:
            cpu: "500m"
            memory: "512Mi"
          requests:
            cpu: "200m"
            memory: "256Mi"
\end{lstlisting}

Powyższy listing \ref{lst:example-k8s} przedstawia prosty manifest Kubernetes wygenerowany przez LLM, który definiuje wdrożenie aplikacji z trzema replikami, ograniczeniami zasobów i ekspozycją portu.

Po omówieniu możliwości generowania kodu, warto przyjrzeć się kwestiom jego jakości i dalszego utrzymania.

\subsection{Wykrywanie i naprawa błędnych konfiguracji przy użyciu LLM}

Oprócz generowania nowych konfiguracji, istotnym polem badań jest wykorzystanie LLM do wykrywania i naprawy błędów w istniejących skryptach IaC. Błędne lub nieoptymalne konfiguracje mogą prowadzić do poważnych problemów – od awarii usług, po luki bezpieczeństwa narażające system na ataki. Tradycyjne narzędzia (skanery statyczne, lintery) potrafią wykryć pewne klasy błędów konfiguracyjnych, lecz ich naprawa zwykle spoczywa na inżynierach. Zainteresowanie budzi więc pytanie, czy model językowy mógłby automatycznie poprawiać wykryte nieprawidłowości IaC.

Low i in. (2024) \cite{low_repairing_2024} przeprowadzili badania nad automatycznym naprawianiem skryptów Terraform (IaC dla chmury) z wykorzystaniem modeli GPT-3.5 oraz GPT-4. Zaprezentowali oni podejście, w którym LLM otrzymuje jako wejście kod infrastruktury oraz raport z narzędzia skanującego (np. listę wykrytych podatności czy niezgodności z politykami bezpieczeństwa) i na tej podstawie generuje poprawioną wersję kodu. Co ważne, autorzy wprowadzili strategię "human-in-the-loop", w której proces naprawy odbywa się w dwóch etapach: w pierwszym model stara się samodzielnie usunąć większość problemów, a w drugim programista może dostarczyć dodatkowych informacji lub kontekstu, aby pomóc modelowi naprawić trudniejsze błędy. Taka iteracyjna pętla ma na celu uzupełnienie brakującej wiedzy – wiele błędów IaC wymaga bowiem informacji o zewnętrznym kontekście (np. identyfikatorów zasobów w chmurze), które nie są zawarte bezpośrednio w kodzie.

Eksperymenty autorów wykazały, że model GPT-4 znacząco przewyższa GPT-3.5 w skuteczności naprawy błędów konfiguracyjnych. Już po pierwszym przejściu GPT-4 usuwał średnio 18–34\% więcej wykrytych niezgodności niż GPT-3.5, a po drugim przejściu różnica ta sięgała nawet 57\% na korzyść GPT-4 (dla niektórych narzędzi skanujących). W najlepszym scenariuszu model GPT-4 był w stanie zredukować liczbę alarmów o błędnej konfiguracji aż o 84,7\% w stosunku do stanu początkowego, znacząco odciążając programistę. Uwzględnienie interwencji człowieka (czyli przekazanie modelowi dodatkowego kontekstu w drugiej iteracji) podniosło odsetek poprawnie usuniętych błędów łącznie o dalsze 22–60\%, osiągając maksymalnie 87,4\% naprawionych mis-konfiguracji (dla problemów wykrywanych przez skaner Checkov \cite{checkov}).

Mimo tych imponujących wyników, analiza jakości wygenerowanych poprawek ujawniła pewne ograniczenia i wyzwania. Około 20,4\% zaproponowanych przez model poprawek okazało się nietrafnych – zawierały one błędy składniowe lub nie rozwiązywały faktycznego problemu, a jedynie „uciszały” ostrzeżenie skanera powierzchowną zmianą. Stwierdzono, że model ma tendencję do halucynowania pól konfiguracji, tzn. dodawania ustawień, które syntaktycznie pasują do kodu i powodują zniknięcie ostrzeżenia, lecz w rzeczywistości nie mają pokrycia w dokumentacji (nie istnieją) bądź nie poprawiają bezpieczeństwa. Przykładowo, LLM potrafił zaproponować pole, które „naprawia” błąd według skanera, ale to pole nie jest obsługiwane przez dany zasób, przez co wynikowy kod nie przejdzie walidacji schematu lub nadal nie spełnia zamierzonej polityki bezpieczeństwa.

Takie zachowanie jest szczególnie niebezpieczne, gdyż generuje fałszywe poczucie poprawy – konfiguracja wygląda na poprawioną (skanery nie zgłaszają błędu), podczas gdy problem nadal istnieje lub pojawiają się inne błędy. Autorzy zaproponowali kilka potencjalnych rozwiązań tych problemów, w tym bardziej restrykcyjne prompty oraz dodatkowe automatyczne testy wygenerowanych poprawek (np. walidacja syntaktyczna i porównanie z intencją oryginalnego kodu). Praca Low i in. stanowi ważną wskazówkę, że choć LLM potrafią znacznie przyspieszyć naprawę infrastruktury jako kodu, to nie można polegać na nich bezkrytycznie – konieczne jest włączenie mechanizmów weryfikujących i korygujących ich propozycje.

Kolejną kluczową kwestią badaną w literaturze jest bezpieczeństwo konfiguracji Kubernetes generowanych przez LLM. Manifesty K8s mogą zawierać subtelne błędy konfiguracyjne (np. brak ograniczeń zasobów, użycie uprzywilejowanych kontenerów, ekspozycja usług bez autoryzacji itp.), które nie wpływają od razu na działanie aplikacji, ale stanowią poważne ryzyko bezpieczeństwa. Malul i in. (2024) \cite{malul_genkubesec_2024} zajęli się problemem wykrywania i automatycznej naprawy niepoprawnych konfiguracji Kubernetes przy użyciu LLM, przedstawiając kompleksowe rozwiązanie o nazwie GenKubeSec.

W odróżnieniu od wcześniej opisanych podejść, skupionych na generowaniu nowych konfiguracji, GenKubeSec zakłada istnienie pewnej bazy wiedzy o znanych błędach Kubernetes i wykorzystuje model językowy do odnajdywania tych błędów w dostarczonych plikach oraz sugerowania poprawek. Architektura GenKubeSec składa się z trzech głównych komponentów: (1) modułu przygotowania danych, gdzie zebrano obszerny zestaw znanych przypadków błędnych konfiguracji Kubernetes (Kubernetes Configuration Faults, KCF) i zunifikowano ich kategoryzację za pomocą ujednoliconego indeksu mis-konfiguracji (Unified Misconfig Index); (2) GenKubeDetect, czyli modelu LLM dostrojonego (fine-tuned) na zebranych danych, zdolnego do wykrywania różnorodnych błędów w konfiguracji Kubernetes; oraz (3) GenKubeResolve, który wykorzystuje pretrenowany model (bez dodatkowego trenowania) wraz z technikami prompt engineering i few-shot learning, aby dla każdego wykrytego błędu podać jego lokalizację w pliku, zrozumiałe wyjaśnienie oraz zaproponować konkretną poprawkę.

Takie podejście pozwala na kompleksową obsługę problemów – od detekcji po naprawę – w ramach jednego systemu, podczas gdy wcześniejsze prace zazwyczaj koncentrowały się albo na samym wykrywaniu, albo tylko na sugerowaniu poprawek. Efektywność GenKubeSec oceniono porównawczo z istniejącymi narzędziami do statycznej analizy manifestów K8s (m.in. Checkov, KubeLinter \cite{kubelinter}, Terrascan \cite{terrascan}). Wyniki są bardzo obiecujące: dostrojony model GenKubeDetect osiągnął precyzję na poziomie 0,990 oraz czułość (recall) 0,999 w wykrywaniu błędów KCF. Oznacza to, że niemal wszystkie rzeczywiste błędy zostały wykryte (tylko 0,1\% umknęło uwadze modelu), przy znikomym odsetku fałszywych alarmów.

Dla porównania, sumaryczna precyzja trzech popularnych narzędzi opartych na regułach była zbliżona (również około 0,99), ale każde z nich z osobna miało luki (błędy wykryte przez GenKubeSec wykraczały poza ich pokrycie). Świadczy to o ogólnej zdolności modelu do uogólniania wiedzy – jest w stanie wykryć także te niepoprawności, których nie obejmują zdefiniowane statycznie reguły. Co więcej, przy pomocy modułu GenKubeResolve system generował dla każdego błędu klarowne wyjaśnienie i propozycję naprawy, które zostały zweryfikowane przez ekspertów Kubernetes jako trafne i poprawne.

Przykładowo, jeśli w manifeście użyto domyślnej przestrzeni nazw (co jest uznawane za złą praktykę), GenKubeSec wskazywał dokładnie tę linię, wyjaśniał dlaczego domyślna przestrzeń nie powinna być używana i sugerował dodanie konkretnej deklaracji namespace lub zmianę na dedykowaną przestrzeń.

Istotnym elementem wdrożeniowym GenKubeSec jest to, że działa on w oparciu o lokalnie uruchomiony model LLM, a nie poprzez zapytania do zewnętrznego API modelu. Dzięki temu rozwiązanie minimalizuje ryzyko wycieku danych wrażliwych – pliki konfiguracyjne (które mogą zawierać np. informacje o architekturze systemu, nazwy usług, a nawet niektóre dane dostępowe) nie są nigdzie wysyłane, co mogłoby stwarzać zagrożenie bezpieczeństwa lub prywatności. Twórcy podkreślają tę zaletę, zauważając że wiele firm obawia się korzystać z chmurowych API LLM właśnie z uwagi na konieczność przesyłania swojej konfiguracji na zewnątrz. GenKubeSec pokazuje, że możliwe jest skuteczne połączenie technik uczenia maszynowego z praktycznymi wymaganiami bezpieczeństwa w środowisku korporacyjnym.

Obserwacje z tych badań posłużą jako podstawa do zaprojektowania mechanizmu iteracyjnej walidacji i korekty generowanych konfiguracji w niniejszej pracy.

\subsection{Bezpieczeństwo zastosowań LLM w IaC}

Bezpieczeństwo jest jednym z kluczowych aspektów przy wdrażaniu dużych modeli językowych do praktyk DevOps i Infrastructure as Code. W literaturze wskazuje się szereg zagrożeń, które mogą pojawić się zarówno po stronie jakości generowanego kodu, jak i podatności samych aplikacji wykorzystujących LLM.

Fu i in. (2025) \cite{fu_security_2025} przeprowadzili analizę bezpieczeństwa kodu generowanego przez LLM na podstawie repozytoriów GitHub. Wykazali oni, że znaczący odsetek fragmentów kodu zawiera podatności bezpieczeństwa, w tym takie, które zostały zaklasyfikowane do listy CWE Top-25 \cite{cwetop25}. Wyniki te wskazują na potrzebę weryfikacji nie tylko konfiguracji infrastruktury, ale również ogólnej jakości kodu produkowanego przez LLM, który może zostać wdrożony bez uprzedniego audytu.

Liu i in. (2024) \cite{liu_prompt_2024} z kolei zaprezentowali technikę ataku prompt injection w aplikacjach wykorzystujących LLM, wskazując na potencjalne wektory zagrożeń, które mogą zostać wykorzystane do manipulacji generowanymi wynikami. Jest to szczególnie istotne w kontekście aplikacji DevOps, gdzie złośliwy użytkownik mógłby wpłynąć na wygenerowaną konfigurację systemu, np. poprzez wstrzyknięcie niepożądanych instrukcji do promptu lub danych wejściowych.

Opisane ryzyka podkreślają konieczność uzupełnienia procesu integracji LLM w IaC o dodatkowe mechanizmy zabezpieczające – takie jak lokalna walidacja, sandboxing, inspekcja promptów, czy użycie oddzielnych mechanizmów inspekcji i audytu konfiguracji po ich wygenerowaniu.

\subsection{Wyzwania i ograniczenia}

Na podstawie przeglądu literatury można stwierdzić, że zastosowanie dużych modeli językowych w dziedzinie IaC niesie ze sobą znaczące korzyści, ale także wiąże się z istotnymi wyzwaniami. Do głównych zalet zaliczymy:

\textbf{Automatyzacja i oszczędność czasu:} LLM są w stanie automatycznie wygenerować znaczną część kodu konfiguracyjnego, począwszy od definicji obrazu Docker, po złożone manifesty Kubernetes. Umożliwia to przyspieszenie wdrożeń – konfiguracje, które normalnie zajęłyby godziny ręcznego pisania i debugowania, mogą zostać uzyskane w ciągu minut. Przykładowo, projekt Repo2Run pokazał, że LLM może samodzielnie przygotować Dockerfile dla skomplikowanego projektu, eliminując dziesiątki potencjalnych kroków manualnych \cite{hu_llm-based_2025}. Automatyzacja przekłada się również na odciążenie specjalistów: deweloper bez głębokiej wiedzy o Kubernetes może uzyskać działający manifest, skupiając się na wysokopoziomowych wymaganiach, podczas gdy szczegółowy kod wygeneruje za niego model. To obniża próg wejścia – zespoły mogą szybciej adoptować technologie chmurowe bez długiego szkolenia z każdego narzędzia.

\textbf{Poprawa produktywności i jakości:} Dzięki LLM możliwe jest wygenerowanie szablonów konfiguracji zgodnych z ogólnie przyjętymi wzorcami i dobrymi praktykami. Dobrze skonstruowany prompt może spowodować, że model uwzględni zalecane ustawienia bezpieczeństwa czy optymalizacji (np. doda limity zasobów w manifeście, utworzy oddzielne sieci dla kontenerów, itp.). W badaniach odnotowano, że LLM potrafią uzupełniać brakujące elementy konfiguracji, zmniejszając ryzyko pominięcia istotnego parametru \cite{kratzke_dont_2024}. Ponadto modele takie jak GPT-4 wykazały zdolność do korygowania istniejącego kodu – co oznacza, że mogą nie tylko generować, ale i naprawiać konfiguracje, redukując liczbę błędów przed wdrożeniem \cite{low_repairing_2024}. To wszystko przekłada się na wyższą jakość infrastruktury: bardziej spójne, przemyślane i bezpieczne konfiguracje.

\textbf{Ujednolicenie i dokumentacja wiedzy:} LLM trenują się na ogromnych zbiorach danych, które zawierają również wiedzę ekspercką rozsianą po dokumentacjach, blogach, forach. Wykorzystując model, możemy niejako skondensować tę wiedzę w generowanych konfiguracjach. Modele mogą wskazywać rozwiązania podpatrzone w wielu źródłach, co jest korzystne zwłaszcza w małych zespołach bez dedykowanych specjalistów od DevOps. Dodatkowo, niektóre prace sugerują, że generowane przez LLM wyjaśnienia (np. moduł GenKubeResolve) mogą służyć jako dokumentacja edukacyjna – model tłumaczy, dlaczego coś jest błędem i jak to naprawić \cite{malul_genkubesec_2024}. To może pomóc zespołom lepiej zrozumieć własną infrastrukturę i uczyć się na bieżąco dobrych praktyk.

Z drugiej strony, pojawia się szereg wyzwań i ryzyk, które należy rozważyć wdrażając LLM do generowania IaC:

\textbf{Halucynacje i błędne konfiguracje:} Duże modele językowe czasem z dużą pewnością podają informacje nieprawdziwe lub nieadekwatne. W kontekście IaC może to oznaczać wygenerowanie parametrów, które wydają się poprawne, ale w rzeczywistości są błędne lub nieistnieją w danej technologii (np. wymyślone pola w manifeście Kubernetes czy atrybuty zasobów Terraform). Tego typu halucynacje są groźne, ponieważ mogą nie zostać natychmiast wychwycone – plik konfiguracyjny może przejść prostą walidację składni, ale dopiero przy wdrożeniu ujawni się problem, albo co gorsza, błąd pozostanie ukryty jako „tykająca bomba”. Rozwiązaniem może być wzbogacenie procesu generowania o walidatory, testy lub drugi model weryfikujący, jednak zwiększa to złożoność całego potoku wytwórczego \cite{low_repairing_2024}.

\textbf{Brak gwarancji spełnienia wymagań:} Obecne modele działają jak „czarne skrzynki” – trudno przewidzieć, czy wygenerowany kod w pełni spełni założenia użytkownika. Ueno i Uchiumi \cite{ueno_migrating_2024} wskazali, że brak jest mechanizmu pewnej weryfikacji, czy output modelu pokrywa wszystkie elementy specyfikacji wejściowej i czyni to we właściwy sposób. W praktyce oznacza to konieczność ręcznego przeglądu i testów. Nadal jednak odpowiedzialność za ostateczną poprawność spoczywa na człowieku. LLM mogą nie uwzględnić pewnych niuansów biznesowych czy kontekstowych, których nie było w opisie – np. mogą domyślnie otworzyć port usługi, mimo że według wymagań bezpieczeństwa powinien on być ograniczony.

\textbf{Bezpieczeństwo i zgodność:} Paradoksalnie, narzędzia służące poprawie bezpieczeństwa (jak GenKubeSec) same muszą być zaadresowane pod kątem bezpieczeństwa. Jedno zagrożenie to kwestia poufności danych – jeśli korzystamy z zewnętrznej usługi LLM (np. API chmurowego), musimy wysłać do niej nasze pliki konfiguracyjne, które mogą zawierać wrażliwe informacje o systemie. W środowiskach korporacyjnych często jest to nieakceptowalne, dlatego preferowane może być użycie lokalnych instancji modeli \cite{malul_genkubesec_2024}. Kolejna sprawa to bezpieczeństwo generowanych konfiguracji: model może nieświadomie zaproponować ustawienia z lukami (np. kontener uruchamiany z uprawnieniami root, brak szyfrowania komunikacji, itp.). Wygenerowany kod koniecznie musi przejść przez standardowe procedury audytu bezpieczeństwa tak samo, jak kod pisany ręcznie.

\textbf{Niezawodność i deterministyczność:} Modele językowe mogą dawać różne wyniki na podstawie nawet subtelnych różnic w promptach. Oznacza to, że proces generowania konfiguracji może być nie zawsze powtarzalny – dwie inferencje mogą wygenerować nieco inny manifest. W kontekście IaC oczekujemy deterministycznych rezultatów. Konieczne jest więc odpowiednie strojenie parametrów modelu (np. temperature=0) oraz standaryzacja promptów, by wyniki były jak najbardziej spójne i przewidywalne. Dodatkowo, jeśli integrujemy LLM w potoki CI/CD, musimy brać pod uwagę jego dostępność i czas odpowiedzi – model pracujący na dużym manifeście może mieć opóźnienia lub być ograniczony limitami API.

\subsection{Podsumowanie}

Podsumowując, literatura wskazuje, że duże modele językowe posiadają już zdolności umożliwiające znaczne usprawnienie procesu tworzenia i utrzymania infrastruktury jako kodu. Udane demonstracje – takie jak automatyczne wygenerowanie działającego Dockerfile dla setek projektów \cite{hu_llm-based_2025}, czy wygenerowanie manifestu Kubernetes pozwalającego na wdrożenie aplikacji bez ręcznej konfiguracji \cite{kratzke_dont_2024} – dowodzą, że automatyzacja DevOps z pomocą AI jest możliwa i może przynieść realne oszczędności czasu oraz redukcję błędów.

Jednocześnie jednak żadne z rozwiązań nie jest pozbawione wad. Wyzwania związane z halucynacjami modeli \cite{low_repairing_2024}, koniecznością walidacji wyników \cite{ueno_migrating_2024, kon_iac-eval_nodate} i zapewnieniem bezpieczeństwa \cite{malul_genkubesec_2024, fu_security_2025} wskazują, że rola człowieka (eksperta DevOps) wciąż pozostaje ważna jako nadzorcy i korektora działania AI. Dlatego bieżące badania kierują się ku metodom łączenia silnych stron LLM z mechanizmami kontrolnymi – np. stosowanie podejścia human-in-the-loop \cite{low_repairing_2024}, rozwijanie benchmarków oceny jakości (aby móc mierzyć postępy i porównywać modele) \cite{ueno_migrating_2024, kon_iac-eval_nodate}, eksplorowanie technik prompt engineering \cite{kratzke_dont_2024, pujar_invited_2023}, a także opracowanie metod ochrony przed atakami prompt injection \cite{liu_prompt_2024}, które zmniejszą skłonność modeli do popełniania typowych błędów lub podatności.

Niniejsza praca magisterska wpisuje się w ten nurt, koncentrując się na zbudowaniu automatycznego narzędzia do generowania konfiguracji Docker i Kubernetes wyłącznie na podstawie repozytorium kodu źródłowego, bez udziału człowieka. W ramach pracy zostanie przeprowadzona analiza i porównanie skuteczności różnych dużych modeli językowych w tym zadaniu. Szczególny nacisk położony będzie na bezpieczeństwo generowanej konfiguracji, poprawność składniową i funkcjonalną manifestów oraz na analizę potencjalnych zagrożeń, takich jak ataki typu prompt injection czy przez złośliwie spreparowane repozytoria. Ostatecznie projekt zostanie rozszerzony o prototyp systemu typu PaaS, umożliwiającego automatyczne wdrażanie aplikacji na podstawie kodu źródłowego, bez konieczności ingerencji człowieka.