\clearpage % Rozdziały zaczynamy od nowej strony.
\section{Projekt eksperymentów}

\subsection{Cel badania}

Celem eksperymentów jest ocena możliwości autonomicznego generowania konfiguracji Infrastructure as Code przez agenty oparte na dużych modelach językowych. Badanie koncentruje się na weryfikacji, czy nowoczesne modele językowe są w stanie — bez ingerencji człowieka — wygenerować funkcjonalne, bezpieczne i zgodne z dobrymi praktykami konfiguracje wdrożeniowe (pliki \texttt{Dockerfile} oraz manifesty Kubernetes) na podstawie wyłącznie analizy struktury i zawartości kodu źródłowego repozytorium.

Eksperymenty obejmują pięć hipotez badawczych (H1–H5) dotyczących funkcjonalności, ograniczeń złożonościowych, jakości i bezpieczeństwa, deterministyczności oraz podatności na manipulację. Każda hipoteza weryfikowana jest przy użyciu dedykowanej metodyki testowej, zestawu metryk oraz wyraźnie zdefiniowanych kryteriów sukcesu lub porażki. Kluczowe pytania to: czy agenty LLM potrafią wygenerować działające konfiguracje, jakie są ograniczenia związane ze złożonością aplikacji, czy generowane konfiguracje wymagają dodatkowej walidacji przed wdrożeniem produkcyjnym, na ile powtarzalny jest proces generacji oraz czy agenty są podatne na manipulację poprzez zawartość repozytorium?

\subsection{Hipotezy badawcze}

W ramach badania sformułowano pięć hipotez weryfikowanych eksperymentalnie:

\begin{itemize}
    \item \textbf{H1: Autonomiczna generacja funkcjonalnych konfiguracji} \\
    Duże modele językowe, działające jako agenty z dostępem do narzędzi analizy repozytorium, są w stanie autonomicznie wygenerować funkcjonalne konfiguracje Docker i Kubernetes, które umożliwiają poprawne uruchomienie aplikacji bez dodatkowej ingerencji człowieka, w tym osiągnięcie gotowości usługowej widocznej na poziomie interfejsu (np. webowe API odpowiada na zapytania HTTP po wdrożeniu na klastrze testowym).

    \item \textbf{H2: Ograniczenia złożonościowe} \\
    Istnieje próg złożoności aplikacji, powyżej którego jakość autonomicznie generowanych konfiguracji znacząco spada. Progi te można uchwycić, obserwując wyniki na kontrolowanych repozytoriach o rosnącej liczbie komponentów i zależności (od prostego monolitu po prosty układ mikroserwisowy); oczekuje się, że metryki sukcesu (np. odsetek poprawnych wdrożeń) maleją wraz ze wzrostem złożoności.

    \item \textbf{H3: Jakość i zgodność z dobrymi praktykami} \\
    Bez dodatkowych instrukcji ukierunkowanych na najlepsze praktyki DevOps, co najmniej 60\% autonomicznie generowanych konfiguracji zawiera błędy lub ostrzeżenia wykrywane przez narzędzia statycznej analizy (Hadolint dla Docker, Kube-linter dla Kubernetes), co wymaga dodatkowej walidacji i poprawek przed wdrożeniem produkcyjnym. Zastosowanie prompt engineeringu (w postaci wyraźnych, konkretnych wytycznych dotyczących dobrych praktyk) zmniejsza liczbę ostrzeżeń i zwiększa odsetek konfiguracji bez ostrzeżeń.

    \item \textbf{H4: Niezawodność i powtarzalność procesu agentowego} \\
    Proces generacji konfiguracji przez agenta LLM charakteryzuje się zmiennością wyników mimo deterministycznych parametrów (\texttt{temperature = 0}, stały seed); przy wielokrotnym uruchomieniu na tym samym repozytorium wskaźniki takie jak \textit{run-to-run diff ratio} (liczba zmienionych linii pomiędzy parami wygenerowanych konfiguracji znormalizowana przez ich długość) lub odchylenie standardowe liczby kroków narzędziowych w sekwencjach zapisanych w LangSmith (różna liczba wywołań funkcji pomocniczych) przekraczają ustalony próg (np. $>0{,}2$), co świadczy o ograniczonej powtarzalności procesu.

    \item \textbf{H5: Podatność na manipulację kontekstem} \\
    Agenty LLM są podatne na manipulację przez złośliwe lub mylące treści zawarte w repozytorium (prompt injection, social engineering w plikach README/dokumentacji): w specjalnie spreparowanym repozytorium co najmniej 30\% przebiegów prowadzi do wygenerowania konfiguracji odbiegających od oczekiwań (np. pominięcie krytycznych usług, wstrzyknięcie dodatkowych kontenerów) lub zawierających intencjonalnie szkodliwe instrukcje, co dokumentuje wpływ manipulacji kontekstem na wynik końcowy.
\end{itemize}

\bigskip
\noindent
Każda z powyższych hipotez weryfikowana jest przy użyciu dedykowanej metodyki testowej, specyficznych zestawów danych, odpowiednich metryk pomiarowych oraz wyraźnie zdefiniowanych kryteriów sukcesu lub porażki.
%
% TODO: Dopisać weryfikację H2--H5 (metodyka, metryki, kryteria).

\subsection{Architektura systemu testowego}

Eksperymenty wymagają kompleksowego środowiska testowego składającego się z trzech głównych komponentów: agenta generującego konfiguracje, potoku walidacyjnego oraz infrastruktury wykonawczej.

\subsubsection{Agent LLM}

Agent został zaimplementowany przy użyciu bibliotek LangChain oraz LangGraph i pełni rolę autonomicznego generatora konfiguracji. Otrzymuje on link do repozytorium Git oraz systemowy prompt z instrukcjami i ma za zadanie wygenerować kompletne pliki \texttt{Dockerfile} oraz manifesty Kubernetes.

\bigskip
\noindent
\textbf{Narzędzia dostępne dla agenta:}

Agent dysponuje zestawem jedenastu funkcji umożliwiających interakcję z repozytorium:

\begin{itemize}
    \item \texttt{clone\_repo(repo\_url)} – klonuje repozytorium Git do izolowanej przestrzeni roboczej i usuwa wybrane pliki uznane za mylące dla generacji (m.in. metadane VCS, pliki CI/CD, pliki dokumentacji, oraz istniejące konfiguracje takie jak \texttt{Dockerfile} i \texttt{docker-compose*.yml}), aby agent generował konfigurację na podstawie analizy kodu źródłowego,

    \item \texttt{prepare\_repo\_tree()} – generuje tekstowy widok struktury repozytorium (lista plików wraz z lokalizacją i rozmiarem), ułatwiający szybkie rozpoznanie kluczowych komponentów,

    \item \texttt{ls(dir\_path)} – listuje zawartość katalogu,

    \item \texttt{get\_file\_content(file\_path)} – odczytuje treść pliku,

    \item \texttt{search\_files(pattern, file\_pattern, case\_sensitive)} – wyszukuje wzorzec tekstowy w plikach,

    \item \texttt{find\_files(pattern, max\_results)} – wyszukuje pliki według wzorca nazwy,

    \item \texttt{write\_file(file\_path, content)} – zapisuje nowy plik lub nadpisuje istniejący,

    \item \texttt{patch\_file(file\_path, patch)} – aplikuje patch w formacie unified diff,

    \item \texttt{base64\_encode(content)} – koduje tekst do base64 (np. dla Kubernetes Secrets),

    \item \texttt{base64\_decode(encoded)} – dekoduje base64,

    \item \texttt{think(thoughts)} – narzędzie refleksyjne pozwalające agentowi na zorganizowanie obserwacji o architekturze (nie modyfikuje plików).
\end{itemize}

\bigskip
\noindent
\textbf{Prompt systemowy:}

Ze względu na długość, pełna treść prompta znajduje się w załączniku~\ref{att:prompt}. Prompt zawiera:
\begin{itemize}
    \item kontekst działania agenta i jego specjalizację,
    \item opis oczekiwanego sposobu pracy (klonowanie repozytorium, analiza kodu i zależności oraz generacja plików konfiguracyjnych),
    \item reguły nazewnictwa (m.in. host Ingress w formacie \texttt{<repository-name>.{domain\_suffix}}; nazwa repozytorium jest zamieniana na małe litery i musi spełniać reguły RFC 1123),
    \item wytyczne wdrożeniowe dla Docker i Kubernetes (m.in. dobór obrazu bazowego, zgodność portów, \texttt{imagePullPolicy: IfNotPresent}, requests/limits, health checks, zasoby typu Deployment/StatefulSet oraz ConfigMap/Secret/PVC dla wykrytych zależności),
    \item format strukturalnego wyjścia (JSON z listą obrazów Docker, manifestów K8s oraz test endpoint).
\end{itemize}

Prompt został sformułowany deklaratywnie, aby maksymalnie zredukować niejednoznaczność interpretacyjną i zwiększyć powtarzalność wyników.

\bigskip
\noindent
\textbf{Strukturalne wyjście:}

Agent po zakończeniu pracy zwraca JSON o następującej strukturze:
\begin{itemize}
    \item \texttt{docker\_images} – lista obiektów z informacją o Dockerfile, tagu obrazu i kontekście budowania,
    \item \texttt{kubernetes\_files} – lista ścieżek do manifestów Kubernetes,
    \item \texttt{test\_endpoint} – relatywna ścieżka HTTP do weryfikacji działania aplikacji.
\end{itemize}

\subsubsection{Potok walidacyjny}

Po wygenerowaniu konfiguracji przez agenta, następuje automatyczna ewaluacja składająca się z niezależnych kroków:

\begin{enumerate}
    \item Weryfikacja syntaktyczna \texttt{Dockerfile} (\texttt{docker build --check}),
    \item Statyczna analiza \texttt{Dockerfile} (Hadolint),
    \item Budowa obrazów Docker oraz publikacja do lokalnego rejestru,
    \item Weryfikacja syntaktyczna manifestów Kubernetes (\texttt{kubectl apply --dry-run=server}),
    \item Statyczna analiza manifestów Kubernetes (Kube-linter),
    \item Aplikowanie manifestów do klastra Kubernetes (MicroK8s) w osobnej przestrzeni nazw dla przebiegu,
    \item Walidacja runtime — sprawdzenie dostępności aplikacji po wdrożeniu (żądanie HTTP do endpointu wskazanego przez agenta w polu \texttt{test\_endpoint}).
\end{enumerate}

Modułowa architektura umożliwia łatwe dodawanie nowych kroków walidacji. Wywołania zewnętrznych narzędzi rejestrują m.in. czas wykonania, status oraz listę wykrytych problemów (z przypisanym poziomem ważności).

\subsubsection{Infrastruktura wykonawcza}

Środowisko testowe składa się z:
\begin{itemize}
    \item \textbf{Klaster Kubernetes}: MicroK8s jako środowisko docelowe dla manifestów,
    \item \textbf{Rejestr Docker}: lokalny rejestr dla przechowywania zbudowanych obrazów,
    \item \textbf{Śledzenie}: LangSmith do inspekcji przebiegów agenta, w tym wywołań narzędzi, promptów i odpowiedzi.
\end{itemize}

\subsubsection{Parametry generacji}

Konfiguracja agenta jest sterowana zestawem parametrów przekazywanych w definicji eksperymentu lub przez zmienne środowiskowe:
\begin{itemize}
    \item \texttt{temperature} — kontroluje losowość generacji; typowo 1.0 (eksploracja) lub 0.0 w testach powtarzalności,
    \item \texttt{seed} — opcjonalne ziarno losowości (np. dla powtarzalnych przebiegów),
    \item \texttt{prompt\_version} — pozwalają wymusić konkretny wariant prompta,
    \item \texttt{recursion\_limit} — limit kroków w pętli agentowej (domyślnie 200),
    \item liczba powtórzeń — ustawiana w definicji eksperymentu (\texttt{repetitions}); określa, ile razy uruchamiany jest ten sam zestaw parametrów (repozytorium, model, prompt/parametry) w celu np. oszacowania zmienności wyników.
\end{itemize}

\subsection{Weryfikacja H1: Autonomiczna generacja funkcjonalnych konfiguracji}

\subsubsection{Hipoteza}

Duże modele językowe, działające jako agenty z dostępem do narzędzi analizy repozytorium, są w stanie autonomicznie wygenerować funkcjonalne konfiguracje Docker i Kubernetes, które umożliwiają poprawne uruchomienie aplikacji bez dodatkowej ingerencji człowieka, w tym osiągnięcie gotowości usługowej widocznej na poziomie interfejsu (np. webowe API odpowiada na zapytania HTTP po wdrożeniu na klastrze testowym).

\subsubsection{Zbiór danych testowych}

Testy H1 opierają się na \textbf{rzeczywistych, publicznie dostępnych projektach z serwisu GitHub}, aby ocenić skuteczność agentów w warunkach zbliżonych do praktycznych. Wykorzystanie niekontrolowanych repozytoriów o zróżnicowanej strukturze, niekompletnej dokumentacji i niestandardowych zależnościach pozwala ocenić, jak agent radzi sobie poza scenariuszami laboratoryjnymi. Zbiór obejmuje 25 aplikacji webowych (załącznik~\ref{att:h1-dataset}) zróżnicowanych pod względem stosu technologicznego, architektury oraz sposobu uruchamiania. Repozytoria zostały zebrane przy użyciu GitHub API z kryteriami:
\begin{itemize}
    \item zapytanie: \texttt{webapp pushed:>=2025-01-01} (ostatni push po 1 stycznia 2025),
    \item języki: Python, JavaScript, TypeScript, Java, Kotlin, C\#, Go, PHP, Ruby (popularne języki stosowane w aplikacjach webowych),
    \item temat: \texttt{webapp},
    \item liczba gwiazdek ("stars"): 0–10 (mniej popularne projekty),
    \item sortowanie: \texttt{pushed} \texttt{desc} (ostatnio aktualizowane).
\end{itemize}

\textbf{Ograniczenia praktyczne:} w badaniu H1 znalazł się zestaw relatywnie prostych aplikacji (mało komponentów, brak skomplikowanej orkiestracji), co obniża koszt eksperymentu, ale ogranicza uogólnienie wyników na złożone systemy — jest to jedno z zagrożeń ważności opisanych w sekcji o threats to validity.

\bigskip
\noindent
\textbf{Konfiguracja eksperymentu H1:} testy uruchamiane są z domyślnym promptem systemowym (\texttt{prompts/default.prompt}, załącznik~\ref{att:prompt}) oraz trzema modelami użytymi w eksperymencie: \texttt{gemini-2.5-flash}, \texttt{gpt-5-mini} i \texttt{deepseek-chat}. Każda kombinacja (repozytorium, model) jest uruchamiana dwukrotnie (2 powtórzenia).

\subsubsection{Metryki}

\begin{itemize}
    \item \texttt{generation\_success} — czy agent zwrócił strukturalny JSON z konfiguracją,
    \item \texttt{dockerfile\_syntax\_valid} — czy pliki Dockerfile są poprawne składniowo,
    \item \texttt{k8s\_syntax\_valid} — czy manifesty Kubernetes są poprawne składniowo,
    \item \texttt{build\_success} — czy obrazy Docker zbudowały się pomyślnie,
    \item \texttt{kubernetes\_apply\_success} — czy manifesty zostały poprawnie zaaplikowane do klastra,
    \item \texttt{runtime\_success} — czy aplikacja uruchomiła się poprawnie w klastrze i odpowiada na żądania HTTP.
\end{itemize}

\textbf{Wskaźnik sukcesu:}
\[
\text{Success rate} = \frac{\text{build\_success} \land \text{kubernetes\_apply\_success} \land \text{runtime\_success}}{\text{total\_runs}} \times 100\%
\]

W analizie dodatkowo raportowany jest spadek skuteczności na kolejnych etapach potoku walidacyjnego (build → apply → runtime) oraz wskaźniki warunkowe (apply | build, runtime | apply), aby wskazać typowe miejsca, w których występują niepowodzenia.

\textbf{Przedziały ufności:}
Dla wskaźników opartych na proporcjach (takich jak ogólny współczynnik sukcesu, metryki dla poszczególnych etapów oraz metryki warunkowe) wyznaczane są \textbf{95\% przedziały ufności metodą Wilsona} \cite{wilson_probable_nodate}.
Przedział ufności określa zakres wartości, w którym -- przy założonym poziomie ufności -- znajduje się rzeczywista wartość mierzonego parametru (np.\ prawdopodobieństwo sukcesu agenta), przy założeniu wielokrotnego powtarzania eksperymentu.

Metoda Wilsona stanowi ulepszenie klasycznego przedziału ufności opartego na przybliżeniu normalnym (tzw.\ przedziału Walda), który przy niewielkiej liczbie obserwacji lub wartościach proporcji bliskich 0 lub 1 może prowadzić do niestabilnych i zbyt wąskich przedziałów.
Przedziały Wilsona charakteryzują się lepszymi własnościami statystycznymi, w szczególności dokładniejszym pokryciem deklarowanego poziomu ufności, oraz zawsze mieszczą się w logicznym zakresie $[0,1]$.
Z tego względu metoda Wilsona jest powszechnie rekomendowana w literaturze jako standardowa metoda wyznaczania przedziałów ufności dla proporcji, zwłaszcza w analizach opartych na ograniczonej liczbie prób.

\subsubsection{Kryteria weryfikacji}

\begin{itemize}
    \item \textbf{Pozytywna weryfikacja hipotezy}: \textit{łączne spełnienie poniższych progów}:
    \begin{itemize}
        \item end-to-end success rate (build \& apply \& runtime) co najmniej 60\% na zbiorze H1,
        \item sukcesy per etap: build $\geq$ 75\%, apply $\geq$ 75\%, runtime $\geq$ 60\%,
        \item conditional apply $|$ build $\geq$ 95\% oraz conditional runtime $|$ apply $\geq$ 80\% (ogranicza wpływ awarii na wcześniejszych etapach),
        \item co najmniej połowa repozytoriów osiąga $\geq$ 50\% sukcesu (obniża ryzyko, że wynik jest zawyżony przez kilka „łatwych” projektów),
    \end{itemize}
\end{itemize}

% TODO dodac opis H2-H5

\subsection{Weryfikacja H2: Ograniczenia złożonościowe}

% TODO: Opisz metodykę (kontrolowane repozytoria o rosnącej złożoności),
% TODO: Zdefiniuj miary złożoności (np. liczba usług, zależności, typ architektury),
% TODO: Określ metryki sukcesu i progi (spadek success rate vs. poziom złożoności),
% TODO: Dodaj kryteria weryfikacji (pozytywna/negatywna).

\subsubsection{Hipoteza}

% TODO: Wstaw treść hipotezy H2 (z sekcji hipotez badawczych).

\subsubsection{Zbiór danych testowych}

% TODO: Opisz zestaw repozytoriów kontrolowanych i sposób eskalacji złożoności.

\subsubsection{Metryki}

% TODO: Zdefiniuj metryki złożoności i metryki skuteczności.

\subsubsection{Kryteria weryfikacji}

% TODO: Zdefiniuj progi i warunki uznania H2.

\subsection{Weryfikacja H3: Jakość i zgodność z dobrymi praktykami}

% TODO: Opisz metodykę (prompt bazowy vs. prompt z dobrymi praktykami),
% TODO: Zdefiniuj sposób liczenia ostrzeżeń Hadolint/Kube-linter,
% TODO: Określ metryki jakości i kryteria sukcesu.

\subsubsection{Hipoteza}

% TODO: Wstaw treść hipotezy H3 (z sekcji hipotez badawczych).

\subsubsection{Zbiór danych testowych}

% TODO: Opisz zbiór (może H1 lub rozszerzony) i warunki uruchomień.

\subsubsection{Metryki}

% TODO: Liczba ostrzeżeń per plik/repo, odsetek konfiguracji bez ostrzeżeń, itp.

\subsubsection{Kryteria weryfikacji}

% TODO: Progi dla spadku ostrzeżeń po prompt engineeringu.

\subsection{Weryfikacja H4: Niezawodność i powtarzalność procesu agentowego}

% TODO: Opisz liczbę powtórzeń, parametry deterministyczne (temperature=0, seed),
% TODO: Zdefiniuj sposób liczenia run-to-run diff ratio i analizę LangSmith.

\subsubsection{Hipoteza}

% TODO: Wstaw treść hipotezy H4 (z sekcji hipotez badawczych).

\subsubsection{Zbiór danych testowych}

% TODO: Opisz repozytoria i liczbę powtórzeń.

\subsubsection{Metryki}

% TODO: run-to-run diff ratio, odchylenie std. liczby kroków narzędziowych, itp.

\subsubsection{Kryteria weryfikacji}

% TODO: Progi dla zmienności (np. diff ratio > 0.2).

\subsection{Weryfikacja H5: Podatność na manipulację kontekstem}

% TODO: Opisz repozytorium z prompt injection i scenariusze manipulacji,
% TODO: Zdefiniuj kryteria "odstępstwa od oczekiwań" i metryki podatności.

\subsubsection{Hipoteza}

% TODO: Wstaw treść hipotezy H5 (z sekcji hipotez badawczych).

\subsubsection{Zbiór danych testowych}

% TODO: Opisz złośliwe repozytorium i warianty ataków.

\subsubsection{Metryki}

% TODO: Odsetek przebiegów z niepożądanymi efektami i typy odchyleń.

\subsubsection{Kryteria weryfikacji}

% TODO: Progi potwierdzające podatność (np. >=30% odchyleń).

\subsection{Orkiestracja i infrastruktura wspierająca}

\subsubsection{System zarządzania eksperymentami}

W celu ułatwienia przeprowadzania i zarządzania eksperymentami zaimplementowano dedykowaną warstwę orkiestracyjną. System umożliwia definiowanie złożonych scenariuszy testowych w formie strukturalnych plików konfiguracyjnych (YAML), które określają macierze kombinacji model × repozytorium, liczbę powtórzeń, warianty promptów oraz wspólne ustawienia środowiskowe.

\bigskip
\noindent
Kluczowe cechy systemu orkiestracji:
\begin{itemize}
    \item \textbf{Deklaratywna konfiguracja eksperymentów} — wszystkie parametry (modele, repozytoria, prompty, liczba powtórzeń) definiowane w plikach YAML,
    \item \textbf{Bezstanowy ewaluator} — komponent ewaluatora pozostaje bezstanowy, co umożliwia wielokrotne użycie w różnych kontekstach,
    \item \textbf{Wsparcie dla wariantów promptów} — możliwość porównywania różnych instrukcji systemowych,
    \item \textbf{Agregacja metryk} — automatyczne zbieranie i utrwalanie wyników w formatach CSV i JSON,
    \item \textbf{Izolacja przestrzeni roboczych} — każdy przebieg operuje we własnym, unikalnym katalogu roboczym.
\end{itemize}

Aby ograniczyć wpływ losowych fluktuacji, proces generacji i ewaluacji może być powtarzany wielokrotnie dla każdej kombinacji (model, repozytorium, prompt).

\subsubsection{Panel sterowania eksperymentami}

W celu usprawnienia zarządzania eksperymentami oraz monitorowania ich postępu zaimplementowano dedykowany interfejs webowy (Streamlit). Panel umożliwia:

\begin{itemize}
    \item \textbf{Wybór i uruchamianie konfiguracji eksperymentów} — graficzny wybór plików YAML i inicjowanie przebiegów,
    \item \textbf{Monitorowanie postępu w czasie rzeczywistym} — aktualny status, liczba ukończonych/pozostałych przebiegów, szacowany czas do zakończenia,
    \item \textbf{Przeglądanie wyników} — podsumowania z podziałem na modele, repozytoria i warianty promptów,
    \item \textbf{Filtrowanie i agregacja danych} — szybkie porównanie wyników i identyfikacja problemów,
    \item \textbf{Dostęp do artefaktów} — bezpośredni wgląd w wygenerowane pliki konfiguracyjne oraz raporty z walidacji.
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{tex/img/streamlit1.png}
  \caption{Widok panelu Streamlit używanego do uruchamiania i monitorowania eksperymentów.}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{tex/img/streamlit2.png}
  \caption{Przykładowy widok artefaktów dla pojedynczego przebiegu (dostęp do wygenerowanych plików).}
\end{figure}

\subsection{Zagrożenia ważności eksperymentów}
\begin{itemize}
    \item \textbf{Reprezentatywność zbioru H1}: 25 relatywnie prostych aplikacji webowych może nie odzwierciedlać złożoności systemów produkcyjnych; wyniki mogą nie uogólniać się na złożone architektury (odniesienie w H2).
    \item \textbf{Walidacja runtime jednym endpointem}: sprawdzenie dostępności pojedynczego \texttt{test\_endpoint} nie weryfikuje w pełni komunikacji między usługami ani scenariuszy biznesowych; złożone zależności mogą pozostać niewykryte.
    \item \textbf{Ograniczona liczba powtórzeń}: 2 przebiegi na kombinację (repozytorium, model) dają szerokie przedziały ufności.
    \item \textbf{Środowisko testowe}: MicroK8s i lokalny rejestr na jednej maszynie mogą różnić się od produkcyjnych klastrów (wydajność, sieć, storage); wyniki mogą być wrażliwe na konfigurację hosta.
    \item \textbf{Czyszczenie repozytorium}: usuwanie istniejących \texttt{Dockerfile}/\texttt{docker-compose*} eliminuje podpowiedzi dla modelu, ale też odbiega od scenariusza, w którym część konfiguracji już istnieje.
\end{itemize}
