\clearpage % Rozdziały zaczynamy od nowej strony.

\section{Przegląd technologii i narzędzi}

Celem tego rozdziału jest przedstawienie najważniejszych technologii i narzędzi, stanowiących podstawę niniejszego badania. Szczególny nacisk położony zostanie na duże modele językowe (LLM), opis ich architektur oraz ich zastosowanie w generowaniu konfiguracji Infrastructure as Code (IaC). Ponadto, omówione zostaną istotne narzędzia DevOps, takie jak Docker i Kubernetes, niezbędne w zarządzaniu środowiskami uruchomieniowymi, oraz narzędzia służące do walidacji i oceny jakości generowanego kodu IaC. Rozdział zawiera także opis środowiska eksperymentalnego oraz wykorzystywanych technologii wspierających automatyzację i monitorowanie eksperymentów.

\subsection{Modele językowe wykorzystywane w badaniu}

Duże modele językowe (LLM) pełnią istotną rolę w prezentowanych eksperymentach, służąc jako generatory konfiguracji IaC. Ich zdolność do przetwarzania języka naturalnego oraz generowania poprawnego kodu czyni je ważnym elementem automatyzacji procesów DevOps. W tym podrozdziale przedstawiono charakterystykę wybranych modeli, opis ich architektur oraz praktyczne aspekty związane z ich wykorzystaniem.

Rynek dużych modeli językowych rozwija się dynamicznie, oferując zarówno modele komercyjne, dostępne przez API, jak i modele open-source, możliwe do uruchomienia lokalnie lub na własnej infrastrukturze. Decyzja o wyborze konkretnego rodzaju modelu często zależy od dostępności zasobów sprzętowych (zwłaszcza GPU), wymagań licencyjnych, kwestii związanych z prywatnością danych oraz możliwości dostosowania modeli do specyficznych zastosowań.

Modele komercyjne dostępne przez API zazwyczaj cechują się wysoką wydajnością, zaawansowanymi możliwościami oraz niezawodnością, dzięki wsparciu dużych firm technologicznych takich jak OpenAI, Anthropic czy Google. Modele te trenowane są na szerokich i zróżnicowanych zbiorach danych, co przekłada się na ich zdolność generowania wysokiej jakości tekstu oraz kodu. Ich użytkowanie wiąże się z kosztami dostępu do API oraz koniecznością przesyłania danych do zewnętrznych serwisów, co może być problematyczne w przypadku danych wrażliwych.

Wybrane do badania modele komercyjne to:

\begin{itemize}
	\item \textbf{OpenAI GPT-4.1}, \textbf{GPT-4o} oraz \textbf{O3}: Najnowsze flagowe modele OpenAI. GPT-4.1 charakteryzuje się udoskonalonymi możliwościami generowania i rozumienia złożonego kodu oraz zoptymalizowanymi funkcjami integracji z narzędziami (tool calling). GPT-4o, model multimodalny, oferuje wysoką wydajność, elastyczność oraz wszechstronne możliwości interakcji. O3 zapewnia optymalne połączenie wydajności, szybkości oraz efektywności kosztowej.
	\item \textbf{Anthropic Claude Opus 4}, \textbf{Sonnet 4} oraz \textbf{Haiku 3.5}: Zaawansowane modele Anthropic, cenione za bezpieczeństwo, zgodność z najlepszymi praktykami AI Safety oraz obsługę długich kontekstów i precyzyjnych instrukcji. Opus 4 to najbardziej zaawansowana wersja, Sonnet 4 równoważy wydajność z szybkością, a Haiku 3.5 skupia się na efektywności kosztowej i szybkości działania.
	\item \textbf{Google Gemini 2.5 Pro} i \textbf{2.5 Flash}: Modele Google z rozszerzonym oknem kontekstu, dedykowane do analizy obszernych repozytoriów kodu i dokumentacji. Wersja Pro oferuje wyjątkową pojemność kontekstu i wysoką wydajność, podczas gdy wersja Flash jest zoptymalizowana pod kątem szybszego działania przy niższych kosztach operacyjnych.
\end{itemize}

Modele open-source stanowią atrakcyjną alternatywę, oferując pełną kontrolę, możliwość uruchomienia na własnej infrastrukturze oraz brak dodatkowych opłat za każde zapytanie. Ich wydajność zależy przede wszystkim od dostępności zasobów sprzętowych, a aktywny rozwój społeczności przyczynia się do ciągłej optymalizacji i pojawiania się coraz bardziej zaawansowanych wersji.

Wybrane do badania modele open-source to:

\begin{itemize}
	\item \textbf{Mistral Medium} oraz \textbf{Codestral}: Modele od firmy Mistral AI, zoptymalizowane pod kątem efektywnego generowania kodu i ogólnego rozumienia kontekstu technicznego. Codestral specjalizuje się w obsłudze złożonych zadań programistycznych, oferując wysoką efektywność przy umiarkowanych wymaganiach sprzętowych.
	\item \textbf{Meta Llama 4 Maverick} oraz \textbf{Llama 4 Scout}: Najnowsze modele open-source od Meta, które stanowią istotny punkt odniesienia w dziedzinie rozwoju sztucznej inteligencji. Maverick wyróżnia się wysokimi osiągami i zaawansowanymi funkcjami, natomiast Scout zapewnia dobrą wydajność przy bardziej ograniczonych zasobach sprzętowych, umożliwiając łatwiejsze wdrożenie w praktycznych zastosowaniach.
	\item \textbf{DeepSeek V3}: Model typu Mixture-of-Experts (MoE), łączący wysoką jakość generowania kodu z dużą efektywnością obliczeniową. DeepSeek V3 wyróżnia się zdolnością do dynamicznego aktywowania tylko części parametrów modelu przy każdym zapytaniu, co pozwala osiągnąć korzystny kompromis między jakością a kosztami obliczeń.
\end{itemize}

Wszystkie wymienione modele reprezentują aktualny szczyt osiągnięć w dziedzinie dużych modeli językowych (LLM). Wybrane zostały przede wszystkim ze względu na ich potwierdzone zdolności do generowania wysokiej jakości kodu oraz wsparcie dla funkcji „tool calling”, które umożliwiają bezpośrednią integrację modeli z zewnętrznymi narzędziami lub usługami poprzez dedykowane mechanizmy API bądź platformy integracyjne, takie jak LangChain czy LangGraph. Funkcje te będą odgrywać znaczącą rolę w proponowanym w tej pracy podejściu opartym na pętli sprzężenia zwrotnego w procesie generowania konfiguracji Infrastructure as Code (IaC).

\subsubsection{Architektury i charakterystyka}

Większość współczesnych dużych modeli językowych bazuje na architekturze transformera \cite{vaswani_attention_2023}, wykorzystując mechanizm uwagi (attention mechanism) do efektywnego przetwarzania sekwencji danych. Architektury te, pomimo wspólnych założeń, różnią się między sobą liczbą parametrów, rozmiarem okna kontekstowego oraz specyficznymi optymalizacjami, które wpływają na ich zdolność do generowania kodu i rozumienia złożonych instrukcji. Coraz większe znaczenie zyskują również alternatywne podejścia architektoniczne, takie jak Mixture-of-Experts (MoE), w których przy każdym zapytaniu aktywowane są tylko wybrane części modelu, co pozwala na zwiększenie efektywności obliczeniowej bez pogarszania jakości generowanych wyników.

\begin{itemize}
	\item \textbf{Liczba parametrów:} Liczba parametrów (często liczona w miliardach) jest wskaźnikiem skali modelu i jego zdolności do uczenia się złożonych wzorców z danych. Modele z większą liczbą parametrów zazwyczaj wykazują lepszą wydajność w szerokim zakresie zadań, jednak ich uruchomienie i obsługa wymagają znacznie większych zasobów obliczeniowych. Należy jednak zaznaczyć, że sama liczba parametrów nie jest jedynym wyznacznikiem jakości; kluczowe znaczenie ma również jakość danych treningowych, architektura (np. Mixture-of-Experts) oraz proces dostrajania modelu.
	\item \textbf{Okno kontekstowe:} Okno kontekstowe odnosi się do maksymalnej długości sekwencji tokenów (słów, znaków, fragmentów kodu), którą model może przetworzyć i wykorzystać do wygenerowania odpowiedzi. Jest to szczególnie istotne w kontekście generowania konfiguracji IaC z repozytoriów kodu. Duże okno kontekstowe pozwala na dostarczenie modelowi całych plików źródłowych aplikacji, fragmentów dokumentacji, wielu powiązanych ze sobą promptów, a nawet wyników walidacji z zewnętrznych narzędzi. To umożliwia agentowi LLM holistyczne zrozumienie projektu i kontekstu, co przekłada się na wyższą jakość i trafność generowanych konfiguracji. Modele takie jak Google Gemini 2.5 Pro wyróżniają się wyjątkowo dużym oknem kontekstowym (do miliona tokenów), co jest znaczącą przewagą w zadaniach wymagających głębokiej analizy kodu i dokumentacji.
	\item \textbf{Specjalizacje i optymalizacje:} Niektóre modele, takie jak Mistral Codestral, są specjalizowane lub fine-tuninguowane w kierunku generowania kodu. Oznacza to, że są one trenowane na dużych zbiorach danych zawierających kod programistyczny, co poprawia ich zdolność do generowania syntaktycznie poprawnego i semantycznie trafnego kodu. Architektury takie jak Mixture-of-Experts (MoE) stosowane w Mixtral 8x7B pozwalają na efektywne skalowanie modeli, aktywując tylko część ekspertów dla danego zapytania, co optymalizuje zużycie zasobów przy zachowaniu wysokiej jakości odpowiedzi.
\end{itemize}

Poniższa tabela \ref{tab:llm-characteristics} przedstawia ogólne charakterystyki wybranych modeli LLM, które będą wykorzystywane w badaniu. Okno kontekstu podano w liczbie tokenów, a liczba parametrów — w miliardach, jeśli została opublikowana.

\begin{table}[!h] \centering
\caption{Charakterystyka wybranych modeli LLM}
\label{tab:llm-characteristic}
\begin{tabular}{| c | c | c | c |} \hline
\textbf{Dostawca} & \textbf{Model} & \textbf{Parametry (mld)} & \textbf{Okno kontekstu (tokeny)} \\ \hline\hline
OpenAI & GPT-4.1 & - & 1M \\ \hline
OpenAI & GPT-4o & - & 128k \\ \hline
OpenAI & O3 & - & 200k \\ \hline
Anthropic & Claude Opus 4 & - & 200k \\ \hline
Anthropic & Claude Sonnet 4 & - & 200k \\ \hline
Anthropic & Claude Haiku 3.5 & - & 200k \\ \hline
Google & Gemini 2.5 Pro & - & 1M \\ \hline
Google & Gemini 2.5 Flash & - & 1M \\ \hline
Mistral AI & Mistral Medium & 12–20 & 32k \\ \hline
Mistral AI & Codestral & 22 & 32k–64k \\ \hline
Meta & Llama 4 Maverick & 70 & 128k \\ \hline
Meta & Llama 4 Scout & 8 & 128k \\ \hline
DeepSeek & DeepSeek V3 & 236 & 128k \\ \hline
\end{tabular}
\end{table}

TODO tu skonczylem

Cel: Przedstawić modele LLM używane w eksperymencie, ich charakterystyki, sposób użycia i ograniczenia.

Proponowana zawartość:
	•	Rodzaje modeli:
	•	    Podział na modele open-source (np. Mistral, LLaMA, Nous Hermes) vs. modele komercyjne dostępne przez API (np. GPT-4, Claude).
	•		Warto podkreślić, że wybór między open-source a komercyjnymi często zależy od dostępności zasobów (GPU), wymagań licencyjnych i elastyczności.
	•	Architektury i charakterystyki:
	•	    Typowe parametry: liczba parametrów, specjalizacje, wersje, okno kontekstu (dlaczego wazne przy repozytoriach kodu)
	•		Warto dodać, że "liczba parametrów" nie zawsze jest jedynym wyznacznikiem jakości, ale jest istotnym parametrem. "Okna kontekstu" – jak najbardziej warto wyjaśnić, dlaczego jest ważne przy repozytoriach kodu (np. umożliwia dostarczanie całych plików, fragmentów dokumentacji, wielu powiązanych promptów jednocześnie).
    •   Wybrane modele do badania:
    •       Krótki opis które i dlaczego z parametrami jakimiś basic
	• 		Informacja, które modele są używane w badaniu i dlaczego (ograniczenia przez tool calling w langgraph).
	•		To jest bardzo ważna informacja! Musisz to jasno przedstawić. Jeśli LangGraph narzuca ograniczenia na wybór modeli (np. tylko te z dobrze zaimplementowanym tool calling), to jest to istotny czynnik wpływający na zakres badań.
	•	Sposób użycia:
	•	    Poprzez OpenRouter, bezpośrednie API, LangChain / LangGraph.
	•	    Sposób integracji z agentami.
	•	Tryby promptowania:
	•	    Zero-shot, few-shot, chain-of-thought, agent-based.
	•	    Porównanie tych podejść i wybór odpowiednich metod do problemu.
	•		Warto wyjaśnić, dlaczego wybrane metody (np. agent-based) są najbardziej odpowiednie dla generowania konfiguracji IaC.
	•	Ograniczenia i praktyczne aspekty:
	•	    Licencje, limity API, ograniczona liczba modeli.
	•	    Context window – jak wpływa na eksperymenty z kodem.
	•	    Brak fine-tuningu – tylko inference.
	•	    Kontrola nad danymi – brak możliwości dostosowania kodu źródłowego w zamkniętych modelach.



\subsection{Narzędzia DevOps: Docker i Kubernetes}

Cel: Opisać narzędzia wykorzystywane do zarządzania środowiskiem uruchomieniowym i ich wpływ na eksperymenty.

Proponowana zawartość:
	•	Docker:
	•	    Do czego jest wykorzystywany: uruchamianie lokalnych usług, środowiska testowe i deweloperskie, konteneryzacja narzędzi, konteneryzacja produkcyjnych aplikacji.
	•	    Jak mozna popelnic bledy
	•		Warto pokazać typowe błędy, które LLM może popełnić (np. użycie ADD zamiast COPY, brak WORKDIR, użycie latest tagu, brak usuwania zależności po buildzie). To od razu pokazuje, dlaczego ocena bezpieczeństwa i optymalizacji jest tak ważna.
	•	Kubernetes:
	•	    Używany z minimalnym zakresem (np. przez Kind, K3s, minikube).
	•	    Rola w testowaniu konfiguracji i IaC.
	•		Warto podkreślić, że Kubernetes służy jako środowisko do walidacji runtime wygenerowanych konfiguracji, a nie tylko do ich tworzenia.
	•	    Jak można popełnić błędy przy generowaniu YAML (niedopasowane zasoby, problemy z dostępnością, błędne konfiguracje).
	•		Dodaj przykłady: brak liveness/readiness probes, źle skonfigurowane serwisy (np. ClusterIP zamiast NodePort/LoadBalancer), brak PersistentVolumeClaim dla baz danych, zbyt wysokie/niskie limity zasobów, brak kontekstu bezpieczeństwa.
	•	Dobre i złe praktyki:
	•	    Przykłady z testów (np. źle ustawione limity zasobów, niepoprawne labele).
	•   Dodac tu Kaniko czy w technologiach wspierajacych?
	•	Chyba nie bedzie wykorzystywane jeszcze tutaj, a jedynie w systemie PaaS



\subsection{Narzędzia do oceny jakości IaC}

Cel: Pokazać narzędzia do analizy i walidacji konfiguracji IaC oraz ich funkcjonalności.

Proponowana zawartość:
	•	Lista narzędzi:
	•	    checkov, kube-linter, kube-score, kubeval, terrascan, cloudeval-yaml, iac-eval.
	•	Opis funkcji i zasad działania:
	•	    Statyczna analiza, reguły zgodności, walidacja schematów.
	•	Przykłady outputów:
	•	    Co zgłaszają narzędzia, jak to interpretować.
	•	    Zestawienie wyników na tych samych plikach (jeśli masz dane).
	•		Pokazać kilka błędów mniejszych i większych, podatności itd



\subsection{Środowisko eksperymentalne}

Cel: Przedstawić infrastrukturę testową i środowisko, w którym uruchamiane będą eksperymenty.

Proponowana zawartość:
	•	Charakterystyka środowiska:
	•	    Lokalna maszyna, Docker Desktop, Kubectl, Kind, Python, LangChain, LangGraph, OpenRouter?.
	•		Jasno określ, co jest na maszynie lokalnej, a co jest usługą zewnętrzną (OpenRouter to API gateway).
	•	Parametry techniczne:
	•	    RAM, CPU, ewentualna obecność GPU, ograniczenia związane z lokalnością - dlaczego lokalnie?
	•	Zasady testowania:
	•	    Jak wygląda testowanie przez API: limity żądań, obsługa błędów, wersje modeli.
	•	    Korzystanie z narzędzi CLI lub SDK.



\subsection{Technologie wspierające (do rozważenia jako część środowiska eksperymentalnego)}

Alternatywa 1: Zostawić jako osobny rozdział z wyraźnym celem: technologie pomocnicze ułatwiające pracę.

Alternatywa 2: Zmergować z poprzednim podrozdziałem jako „środowisko eksperymentalne i wspierające technologie”.

Zawartość:
	•	Język Python:
	•	    Używane biblioteki (np. requests, openai, langchain, pydantic, git, docker-py itd.).
	•	Narzędzia pomocnicze:
	•	    Kind, Lens, VSCode, Docker CLI, kubectl, LangSmith, LangFuse, Promptfoo, Kaniko.
	•		LangSmith, LangFuse, Promptfoo: To jest bardzo ważne! Są to narzędzia do ewaluacji i zarządzania promptami/LLM. Koniecznie je opisz krótko i wyjaśnij, jak pomogły w Twoich badaniach (np. do monitorowania interakcji z LLM, testowania promptów, porównywania ich skuteczności). To pokazuje zaawansowane podejście do inżynierii promptów.
	•	Rola:
	•	    Obsługa eksperymentów, monitorowanie, automatyzacja.


