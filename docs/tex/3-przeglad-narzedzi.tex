\clearpage % Rozdziały zaczynamy od nowej strony.

\section{Przegląd technologii i narzędzi}

Celem tego rozdziału jest przedstawienie najważniejszych technologii i narzędzi, stanowiących podstawę niniejszego badania. Szczególny nacisk położony zostanie na duże modele językowe (LLM), opis ich architektur oraz ich zastosowanie w generowaniu konfiguracji Infrastructure as Code (IaC). Ponadto, omówione zostaną istotne narzędzia DevOps, takie jak Docker i Kubernetes, niezbędne w zarządzaniu środowiskami uruchomieniowymi, oraz narzędzia służące do walidacji i oceny jakości generowanego kodu IaC. Rozdział zawiera także opis środowiska eksperymentalnego oraz wykorzystywanych technologii wspierających automatyzację i monitorowanie eksperymentów.

\subsection{Modele językowe wykorzystywane w badaniu}

Duże modele językowe (LLM) pełnią istotną rolę w prezentowanych eksperymentach, służąc jako generatory konfiguracji IaC. Ich zdolność do przetwarzania języka naturalnego oraz generowania poprawnego kodu czyni je ważnym elementem automatyzacji procesów DevOps. W tym podrozdziale przedstawiono charakterystykę wybranych modeli, opis ich architektur oraz praktyczne aspekty związane z ich wykorzystaniem.

Rynek dużych modeli językowych rozwija się dynamicznie, oferując zarówno modele komercyjne, dostępne przez API, jak i modele open-source, możliwe do uruchomienia lokalnie lub na własnej infrastrukturze. Decyzja o wyborze konkretnego rodzaju modelu często zależy od dostępności zasobów sprzętowych (zwłaszcza GPU), wymagań dotyczących licencji, kwestii związanych z prywatnością danych oraz możliwości dostosowania modeli do specyficznych zastosowań.

Modele komercyjne dostępne przez API zazwyczaj cechują się wysoką wydajnością, zaawansowanymi możliwościami oraz niezawodnością, dzięki wsparciu dużych firm technologicznych takich jak OpenAI, Anthropic czy Google. Modele te trenowane są na szerokich i zróżnicowanych zbiorach danych, co przekłada się na ich zdolność generowania wysokiej jakości tekstu oraz kodu. Ich użytkowanie wiąże się z kosztami dostępu do API oraz koniecznością przesyłania danych do zewnętrznych serwisów, co może być problematyczne w przypadku danych wrażliwych.

Wybrane do badania modele komercyjne to:

\begin{itemize}
    \item \textbf{OpenAI GPT-4.1}, \textbf{GPT-4o} oraz \textbf{O3}: Najnowsze flagowe modele OpenAI. GPT-4.1 charakteryzuje się udoskonalonymi możliwościami generowania i rozumienia złożonego kodu oraz zoptymalizowanymi funkcjami integracji z narzędziami (tool calling). GPT-4o, model multimodalny, oferuje wysoką wydajność, elastyczność oraz wszechstronne możliwości interakcji. O3 zapewnia optymalne połączenie wydajności, szybkości oraz efektywności kosztowej.
    \item \textbf{Anthropic Claude Opus 4}, \textbf{Sonnet 4} oraz \textbf{Haiku 3.5}: Zaawansowane modele Anthropic, cenione za bezpieczeństwo, zgodność z najlepszymi praktykami AI Safety oraz obsługę długich kontekstów i precyzyjnych instrukcji. Opus 4 to najbardziej zaawansowana wersja, Sonnet 4 równoważy wydajność z szybkością, a Haiku 3.5 skupia się na efektywności kosztowej i szybkości działania.
    \item \textbf{Google Gemini 2.5 Pro} i \textbf{2.5 Flash}: Modele Google z rozszerzonym oknem kontekstu, dedykowane do analizy obszernych repozytoriów kodu i dokumentacji. Wersja Pro oferuje wyjątkową pojemność kontekstu i wysoką wydajność, podczas gdy wersja Flash jest zoptymalizowana pod kątem szybszego działania przy niższych kosztach operacyjnych.
\end{itemize}

Modele open-source stanowią atrakcyjną alternatywę, oferując pełną kontrolę, możliwość uruchomienia na własnej infrastrukturze oraz brak dodatkowych opłat za każde zapytanie. Ich wydajność zależy przede wszystkim od dostępności zasobów sprzętowych, a aktywny rozwój społeczności przyczynia się do ciągłej optymalizacji i pojawiania się coraz bardziej zaawansowanych wersji.

Wybrane do badania modele open-source to:

\begin{itemize}
    \item \textbf{Mistral Medium} oraz \textbf{Codestral}: Modele od firmy Mistral AI, zoptymalizowane pod kątem efektywnego generowania kodu i ogólnego rozumienia kontekstu technicznego. Codestral specjalizuje się w obsłudze złożonych zadań programistycznych, oferując wysoką efektywność przy umiarkowanych wymaganiach sprzętowych.
    \item \textbf{Meta Llama 4 Maverick} oraz \textbf{Llama 4 Scout}: Najnowsze modele open-source od Meta, które stanowią istotny punkt odniesienia w dziedzinie rozwoju sztucznej inteligencji. Maverick wyróżnia się wysokimi osiągami i zaawansowanymi funkcjami, natomiast Scout zapewnia dobrą wydajność przy bardziej ograniczonych zasobach sprzętowych, umożliwiając łatwiejsze wdrożenie w praktycznych zastosowaniach.
    \item \textbf{DeepSeek V3}: Model typu Mixture-of-Experts (MoE), łączący wysoką jakość generowania kodu z dużą efektywnością obliczeniową. DeepSeek V3 wyróżnia się zdolnością do dynamicznego aktywowania tylko części parametrów modelu przy każdym zapytaniu, co pozwala osiągnąć korzystny kompromis między jakością a kosztami obliczeń.
\end{itemize}

Wszystkie wymienione modele reprezentują aktualny szczyt osiągnięć w dziedzinie dużych modeli językowych (LLM). Wybrane zostały przede wszystkim ze względu na ich potwierdzone zdolności do generowania wysokiej jakości kodu oraz wsparcie dla funkcji „tool calling”, które umożliwiają bezpośrednią integrację modeli z zewnętrznymi narzędziami lub usługami poprzez dedykowane mechanizmy API bądź platformy integracyjne, takie jak LangChain czy LangGraph. Funkcje te będą odgrywać znaczącą rolę w proponowanym w tej pracy podejściu opartym na pętli sprzężenia zwrotnego w procesie generowania konfiguracji Infrastructure as Code (IaC).

\subsection{Architektury i charakterystyka}

Większość współczesnych dużych modeli językowych bazuje na architekturze transformera \cite{vaswani_attention_2023}, wykorzystując mechanizm uwagi (attention mechanism) do efektywnego przetwarzania sekwencji danych. Architektury te, pomimo wspólnych założeń, różnią się między sobą liczbą parametrów, rozmiarem okna kontekstowego oraz specyficznymi optymalizacjami, które wpływają na ich zdolność do generowania kodu i rozumienia złożonych instrukcji. Coraz większe znaczenie zyskują również alternatywne podejścia architektoniczne, takie jak Mixture-of-Experts (MoE), w których przy każdym zapytaniu aktywowane są tylko wybrane części modelu, co pozwala na zwiększenie efektywności obliczeniowej bez pogarszania jakości generowanych wyników.

\begin{itemize}
    \item \textbf{Liczba parametrów:} Liczba parametrów (często liczona w miliardach) jest wskaźnikiem skali modelu i jego zdolności do uczenia się złożonych wzorców z danych. Modele z większą liczbą parametrów zazwyczaj wykazują lepszą wydajność w szerokim zakresie zadań, jednak ich uruchomienie i obsługa wymagają znacznie większych zasobów obliczeniowych. Należy jednak zaznaczyć, że sama liczba parametrów nie jest jedynym wyznacznikiem jakości; znaczenie ma również jakość danych treningowych, architektura (np. Mixture-of-Experts) oraz proces dostrajania modelu.
    \item \textbf{Okno kontekstowe:} Okno kontekstowe odnosi się do maksymalnej długości sekwencji tokenów (słów, znaków, fragmentów kodu), którą model może przetworzyć i wykorzystać do wygenerowania odpowiedzi. Jest to szczególnie istotne w kontekście generowania konfiguracji IaC z repozytoriów kodu. Duże okno kontekstowe pozwala na dostarczenie modelowi całych plików źródłowych aplikacji, fragmentów dokumentacji, wielu powiązanych ze sobą promptów, a nawet wyników walidacji z zewnętrznych narzędzi. To umożliwia agentowi LLM holistyczne zrozumienie projektu i kontekstu, co przekłada się na wyższą jakość i trafność generowanych konfiguracji. Modele takie jak Google Gemini 2.5 Pro wyróżniają się wyjątkowo dużym oknem kontekstowym (do miliona tokenów), co jest znaczącą przewagą w zadaniach wymagających głębokiej analizy kodu i dokumentacji.
    \item \textbf{Specjalizacje i optymalizacje:} Niektóre modele, takie jak Mistral Codestral, są specjalizowane lub fine-tuninguowane w kierunku generowania kodu. Oznacza to, że są one trenowane na dużych zbiorach danych zawierających kod programistyczny, co poprawia ich zdolność do generowania syntaktycznie poprawnego i semantycznie trafnego kodu. Architektury takie jak Mixture-of-Experts (MoE) stosowane w Mixtral 8x7B (oraz DeepSeek V3) pozwalają na efektywne skalowanie modeli, aktywując tylko część ekspertów dla danego zapytania, co optymalizuje zużycie zasobów przy zachowaniu wysokiej jakości odpowiedzi.
\end{itemize}

Poniższa tabela \ref{tab:llm-characteristic} przedstawia ogólne charakterystyki wybranych modeli LLM, które będą wykorzystywane w badaniu. Okno kontekstu podano w liczbie tokenów, a liczba parametrów — w miliardach, jeśli została opublikowana.

\newpage % zeby wymusic tabelke

\begin{table}[!h] \centering
\caption{Charakterystyka wybranych modeli LLM}
\label{tab:llm-characteristic}
\begin{tabular}{| c | c | c | c |} \hline
\textbf{Dostawca} & \textbf{Model} & \textbf{Parametry (mld)} & \textbf{Okno kontekstu (tokeny)} \\ \hline\hline
OpenAI & GPT-4.1 & - & 1M \\ \hline
OpenAI & GPT-4o & - & 128k \\ \hline
OpenAI & O3 & - & 200k \\ \hline
Anthropic & Claude Opus 4 & - & 200k \\ \hline
Anthropic & Claude Sonnet 4 & - & 200k \\ \hline
Anthropic & Claude Haiku 3.5 & - & 200k \\ \hline
Google & Gemini 2.5 Pro & - & 1M \\ \hline
Google & Gemini 2.5 Flash & - & 1M \\ \hline
Mistral AI & Mistral Medium & 7 & 32k \\ \hline
Mistral AI & Codestral & 22 & 32k–64k \\ \hline
Meta & Llama 4 Maverick & 70 & 128k \\ \hline
Meta & Llama 4 Scout & 8 & 128k \\ \hline
DeepSeek & DeepSeek V3 & 236 & 128k \\ \hline
\end{tabular}
\end{table}

\subsection{Sposoby wykorzystania modeli}

W ramach niniejszego badania modele językowe są wykorzystywane w trybie inferencji (wnioskowania), bez dodatkowego etapu ich dostrajania (fine-tuningu) na specyficznym zbiorze danych IaC. Oznacza to, że modele wykorzystują swoją ogólną wiedzę, nabytą podczas szerokiego treningu, do generowania konfiguracji na podstawie dostarczonego kontekstu i promptów. Taki sposób podejścia jest zgodny z koncepcją "Don't Train, Just Prompt" \cite{kratzke_dont_2024}, co pozwala na szybką iterację i wykorzystanie najnowszych, potężnych modeli bez konieczności kosztownego i czasochłonnego procesu retrenowania.

Integracja z wybranymi modelami LLM odbywa się głównie za pośrednictwem ich bezpośrednich interfejsów programistycznych (API). Takie podejście gwarantuje dostęp do najnowszych wersji modeli, pełną kontrolę nad parametrami zapytania oraz optymalne czasy odpowiedzi. W przypadku modeli open-source, które nie oferują własnego API, wykorzystana została platforma OpenRouter, która agreguje dostęp do wielu modeli od różnych dostawców, oferując ujednolicony interfejs.

TODO troche krotko

\subsection{Tryby promptowania}

Efektywność wykorzystania dużych modeli językowych w dużej mierze zależy od sposobu formułowania zapytań (promptów) \cite{kratzke_dont_2024}. Istnieje kilka podstawowych strategii promptowania, które mogą być stosowane w zależności od złożoności zadania i dostępności przykładów:

\begin{itemize}
	\item \textbf{Zero-shot prompting:} Jest to najprostsza forma promptowania, gdzie model otrzymuje zadanie lub pytanie bez żadnych wcześniejszych przykładów. Oczekuje się, że model, bazując na swojej wiedzy ogólnej, wygeneruje odpowiedź. W kontekście generowania IaC, model otrzymałby instrukcję typu "Wygeneruj Dockerfile dla aplikacji Node.js", bez dodatkowych danych kontekstowych poza samym opisem zadania. Jest to podejście szybkie, ale jego skuteczność jest często ograniczona, zwłaszcza w przypadku złożonych lub bardzo specyficznych wymagań.
	\item \textbf{Few-shot prompting:} Ta strategia polega na dostarczeniu modelowi kilku przykładów par (zapytanie, oczekiwana odpowiedź) przed właściwym zadaniem \cite{brown_language_2020} Przykłady te służą jako demonstracja pożądanego formatu i stylu odpowiedzi, pomagając modelowi zrozumieć intencję użytkownika i dostosować swoje generacje. W przypadku IaC, można by podać kilka par "opis aplikacji \textrightarrow{} przykładowy Dockerfile/manifest Kubernetes", aby model nauczył się wzorca i preferowanych praktyk. Zastosowanie tej metody często zwiększa jakość generacji w porównaniu do zero-shot.
	\item \textbf{Chain-of-Thought (CoT) prompting:} Jest to technika, w której prompt instruuje model, aby przed podaniem ostatecznej odpowiedzi, wygenerował sekwencję pośrednich kroków rozumowania. Model "myśli na głos", co często prowadzi do bardziej precyzyjnych i trafnych wyników, szczególnie w zadaniach wymagających złożonych rozumowań lub wieloetapowych rozwiązań. W początkowej fazie rozwoju LLM technika CoT często wymagała dedykowanych instrukcji w prompcie, aby model wygenerował te kroki. Obecnie wiele zaawansowanych modeli językowych, dzięki swoim architekturze i procesom treningowym, potrafi wykazywać tego typu "rozumowanie" w sposób bardziej intuicyjny, często nawet bez wyraźnego polecenia, jeśli kontekst zadania na to wskazuje. W kontekście IaC, model mógłby najpierw "zastanowić się" nad zależnościami aplikacji, potem nad wymaganiami środowiskowymi i optymalizacjami, a dopiero potem wygenerować kod konfiguracji. CoT poprawia transparentność i debugowalność procesu generowania, a także może zwiększyć poprawność odpowiedzi.
	\item \textbf{Agent-based prompting:} Ten tryb wykracza poza jednorazowe generowanie odpowiedzi. Model, działając jako "agent", otrzymuje zadanie i może iteracyjnie wykonywać szereg akcji: planować, używać zewnętrznych narzędzi (tzw. "tool calling"), analizować ich wyniki i modyfikować swój plan działania w celu osiągnięcia celu. Podejście agentowe można postrzegać jako ewolucję i rozszerzenie idei Chain-of-Thought, gdzie "myślenie" modelu jest przekształcane w sekwencję interakcji ze światem zewnętrznym i pętle sprzężenia zwrotnego. Agent nie tylko generuje wewnętrzne kroki rozumowania, ale także aktywnie wykorzystuje narzędzia do zbierania danych, weryfikacji hipotez i korygowania swojego działania na podstawie rzeczywistych wyników. Jest to podejście szczególnie efektywne w przypadku złożonych problemów wymagających interakcji ze środowiskiem zewnętrznym oraz adaptacji.
\end{itemize}

W kontekście generowania konfiguracji Infrastructure as Code (IaC), zastosowanie podejścia agentowego jest szczególnie uzasadnione z powodu możliwości zastosowania pętli sprzężenia zwrotnego. Generowanie IaC często wymaga wielu prób i korekt, zwłaszcza gdy dąży się do uzyskania poprawnych i bezpiecznych konfiguracji, które są zgodne z rzeczywistym środowiskiem i przestrzegają standardów. W przeciwieństwie do jednorazowych generacji (jak w zero-shot czy few-shot), agent może, na podstawie dostarczonego mu opisu docelowej infrastruktury, wygenerować wstępną wersję konfiguracji (np. Dockerfile). Następnie, kluczowym elementem jest możliwość wykorzystania zewnętrznych narzędzi i interakcji z kontekstem. Agent może na przykład czytać wybrane pliki z repozytorium kodu, aby lepiej zrozumieć strukturę projektu i zależności. Może również sterować swoim wewnętrznym przepływem rozumowania i zakresem czytanego kontekstu. Ponadto, agent jest w stanie próbować budować obraz Docker na podstawie wygenerowanej konfiguracji Dockerfile i sprawdzać, czy proces ten zakończy się powodzeniem. Wyniki tych interakcji – czy to błędy podczas budowania, czy niespójności z oczekiwaniami – są z powrotem przekazywane agentowi. Na podstawie tych informacji, agent może dynamicznie modyfikować swój pierwotny plan lub formułować nowe zapytania do modelu LLM, prosząc o konkretne korekty. Ten cykl generowania, walidacji i korekty może być powtarzany aż do momentu, gdy wygenerowana konfiguracja spełni określone kryteria poprawności i bezpieczeństwa, bądź do wyczerpania ustalonej liczby iteracji. Takie podejście znacząco zwiększa jakość i niezawodność finalnie otrzymanych konfiguracji IaC.

W niniejszej pracy magisterskiej zastosowana zostanie właśnie metoda agentowa. Podejście agentowe może być realizowane na różne sposoby, zarówno poprzez bezpośrednie wykorzystanie dedykowanych interfejsów API udostępnianych przez niektóre modele (np. poprzez \textit{tool calling} czy \textit{function calling}), jak i poprzez zastosowanie specjalizowanych frameworków orkiestracyjnych, takich jak \textbf{AutoGen} czy \textbf{LangChain/LangGraph}. W tym badaniu, do implementacji agentów i zarządzania złożonymi przepływami pracy z wykorzystaniem funkcji \textit{tool calling} wybrana została biblioteka LangGraph w języku Python, uzupełniona o funkcjonalności LangChain.

TODO tu skonczylem

Cel: Przedstawić modele LLM używane w eksperymencie, ich charakterystyki, sposób użycia i ograniczenia.

Proponowana zawartość:
	•	Rodzaje modeli:W
	•	    Podział na modele open-source (np. Mistral, LLaMA, Nous Hermes) vs. modele komercyjne dostępne przez API (np. GPT-4, Claude).
	•		Warto podkreślić, że wybór między open-source a komercyjnymi często zależy od dostępności zasobów (GPU), wymagań licencyjnych i elastyczności.
	•	Architektury i charakterystyki:
	•	    Typowe parametry: liczba parametrów, specjalizacje, wersje, okno kontekstu (dlaczego wazne przy repozytoriach kodu)
	•		Warto dodać, że "liczba parametrów" nie zawsze jest jedynym wyznacznikiem jakości, ale jest istotnym parametrem. "Okna kontekstu" – jak najbardziej warto wyjaśnić, dlaczego jest ważne przy repozytoriach kodu (np. umożliwia dostarczanie całych plików, fragmentów dokumentacji, wielu powiązanych promptów jednocześnie).
    •   Wybrane modele do badania:
    •       Krótki opis które i dlaczego z parametrami jakimiś basic
	• 		Informacja, które modele są używane w badaniu i dlaczego (ograniczenia przez tool calling w langgraph).
	•		To jest bardzo ważna informacja! Musisz to jasno przedstawić. Jeśli LangGraph narzuca ograniczenia na wybór modeli (np. tylko te z dobrze zaimplementowanym tool calling), to jest to istotny czynnik wpływający na zakres badań.
	•	Sposób użycia:
	•	    Poprzez OpenRouter, bezpośrednie API, LangChain / LangGraph.
	•	    Sposób integracji z agentami.
	•	Tryby promptowania:
	•	    Zero-shot, few-shot, chain-of-thought, agent-based.
	•	    Porównanie tych podejść i wybór odpowiednich metod do problemu.
	•		Warto wyjaśnić, dlaczego wybrane metody (np. agent-based) są najbardziej odpowiednie dla generowania konfiguracji IaC.
	•	Ograniczenia i praktyczne aspekty:
	•	    Licencje, limity API, ograniczona liczba modeli.
	•	    Context window – jak wpływa na eksperymenty z kodem.
	•	    Brak fine-tuningu – tylko inference.
	•	    Kontrola nad danymi – brak możliwości dostosowania kodu źródłowego w zamkniętych modelach.



\subsection{Narzędzia DevOps: Docker i Kubernetes}

Cel: Opisać narzędzia wykorzystywane do zarządzania środowiskiem uruchomieniowym i ich wpływ na eksperymenty.

Proponowana zawartość:
	•	Docker:
	•	    Do czego jest wykorzystywany: uruchamianie lokalnych usług, środowiska testowe i deweloperskie, konteneryzacja narzędzi, konteneryzacja produkcyjnych aplikacji.
	•	    Jak mozna popelnic bledy
	•		Warto pokazać typowe błędy, które LLM może popełnić (np. użycie ADD zamiast COPY, brak WORKDIR, użycie latest tagu, brak usuwania zależności po buildzie). To od razu pokazuje, dlaczego ocena bezpieczeństwa i optymalizacji jest tak ważna.
	•	Kubernetes:
	•	    Używany z minimalnym zakresem (np. przez Kind, K3s, minikube).
	•	    Rola w testowaniu konfiguracji i IaC.
	•		Warto podkreślić, że Kubernetes służy jako środowisko do walidacji runtime wygenerowanych konfiguracji, a nie tylko do ich tworzenia.
	•	    Jak można popełnić błędy przy generowaniu YAML (niedopasowane zasoby, problemy z dostępnością, błędne konfiguracje).
	•		Dodaj przykłady: brak liveness/readiness probes, źle skonfigurowane serwisy (np. ClusterIP zamiast NodePort/LoadBalancer), brak PersistentVolumeClaim dla baz danych, zbyt wysokie/niskie limity zasobów, brak kontekstu bezpieczeństwa.
	•	Dobre i złe praktyki:
	•	    Przykłady z testów (np. źle ustawione limity zasobów, niepoprawne labele).
	•   Dodac tu Kaniko czy w technologiach wspierajacych?
	•	Chyba nie bedzie wykorzystywane jeszcze tutaj, a jedynie w systemie PaaS



\subsection{Narzędzia do oceny jakości IaC}

Cel: Pokazać narzędzia do analizy i walidacji konfiguracji IaC oraz ich funkcjonalności.

Proponowana zawartość:
	•	Lista narzędzi:
	•	    checkov, kube-linter, kube-score, kubeval, terrascan, cloudeval-yaml, iac-eval.
	•	Opis funkcji i zasad działania:
	•	    Statyczna analiza, reguły zgodności, walidacja schematów.
	•	Przykłady outputów:
	•	    Co zgłaszają narzędzia, jak to interpretować.
	•	    Zestawienie wyników na tych samych plikach (jeśli masz dane).
	•		Pokazać kilka błędów mniejszych i większych, podatności itd



\subsection{Środowisko eksperymentalne}

Cel: Przedstawić infrastrukturę testową i środowisko, w którym uruchamiane będą eksperymenty.

Proponowana zawartość:
	•	Charakterystyka środowiska:
	•	    Lokalna maszyna, Docker Desktop, Kubectl, Kind, Python, LangChain, LangGraph, OpenRouter?.
	•		Jasno określ, co jest na maszynie lokalnej, a co jest usługą zewnętrzną (OpenRouter to API gateway).
	•	Parametry techniczne:
	•	    RAM, CPU, ewentualna obecność GPU, ograniczenia związane z lokalnością - dlaczego lokalnie?
	•	Zasady testowania:
	•	    Jak wygląda testowanie przez API: limity żądań, obsługa błędów, wersje modeli.
	•	    Korzystanie z narzędzi CLI lub SDK.



\subsection{Technologie wspierające (do rozważenia jako część środowiska eksperymentalnego)}

Alternatywa 1: Zostawić jako osobny rozdział z wyraźnym celem: technologie pomocnicze ułatwiające pracę.

Alternatywa 2: Zmergować z poprzednim podrozdziałem jako „środowisko eksperymentalne i wspierające technologie”.

Zawartość:
	•	Język Python:
	•	    Używane biblioteki (np. requests, openai, langchain, pydantic, git, docker-py itd.).
	•	Narzędzia pomocnicze:
	•	    Kind, Lens, VSCode, Docker CLI, kubectl, LangSmith, LangFuse, Promptfoo, Kaniko.
	•		LangSmith, LangFuse, Promptfoo: To jest bardzo ważne! Są to narzędzia do ewaluacji i zarządzania promptami/LLM. Koniecznie je opisz krótko i wyjaśnij, jak pomogły w Twoich badaniach (np. do monitorowania interakcji z LLM, testowania promptów, porównywania ich skuteczności). To pokazuje zaawansowane podejście do inżynierii promptów.
	•	Rola:
	•	    Obsługa eksperymentów, monitorowanie, automatyzacja.


