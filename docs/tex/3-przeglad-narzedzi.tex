\clearpage % Rozdziały zaczynamy od nowej strony.

\section{Przegląd technologii i narzędzi}

\subsection{Modele językowe wykorzystywane w badaniu}

Cel: Przedstawić modele LLM używane w eksperymencie, ich charakterystyki, sposób użycia i ograniczenia.

Proponowana zawartość:
	•	Rodzaje modeli:
	•	    Podział na modele open-source (np. Mistral, LLaMA, Nous Hermes) vs. modele komercyjne dostępne przez API (np. GPT-4, Claude).
	•		Warto podkreślić, że wybór między open-source a komercyjnymi często zależy od dostępności zasobów (GPU), wymagań licencyjnych i elastyczności.
	•	Architektury i charakterystyki:
	•	    Typowe parametry: liczba parametrów, specjalizacje, wersje, okno kontekstu (dlaczego wazne przy repozytoriach kodu)
	•		Warto dodać, że "liczba parametrów" nie zawsze jest jedynym wyznacznikiem jakości, ale jest istotnym parametrem. "Okna kontekstu" – jak najbardziej warto wyjaśnić, dlaczego jest ważne przy repozytoriach kodu (np. umożliwia dostarczanie całych plików, fragmentów dokumentacji, wielu powiązanych promptów jednocześnie).
    •   Wybrane modele do badania:
    •       Krótki opis które i dlaczego z parametrami jakimiś basic
	• 		Informacja, które modele są używane w badaniu i dlaczego (ograniczenia przez tool calling w langgraph).
	•		To jest bardzo ważna informacja! Musisz to jasno przedstawić. Jeśli LangGraph narzuca ograniczenia na wybór modeli (np. tylko te z dobrze zaimplementowanym tool calling), to jest to istotny czynnik wpływający na zakres badań.
	•	Sposób użycia:
	•	    Poprzez OpenRouter, bezpośrednie API, LangChain / LangGraph.
	•	    Sposób integracji z agentami.
	•	Tryby promptowania:
	•	    Zero-shot, few-shot, chain-of-thought, agent-based.
	•	    Porównanie tych podejść i wybór odpowiednich metod do problemu.
	•		Warto wyjaśnić, dlaczego wybrane metody (np. agent-based) są najbardziej odpowiednie dla generowania konfiguracji IaC.
	•	Ograniczenia i praktyczne aspekty:
	•	    Licencje, limity API, ograniczona liczba modeli.
	•	    Context window – jak wpływa na eksperymenty z kodem.
	•	    Brak fine-tuningu – tylko inference.
	•	    Kontrola nad danymi – brak możliwości dostosowania kodu źródłowego w zamkniętych modelach.



\subsection{Narzędzia DevOps: Docker i Kubernetes}

Cel: Opisać narzędzia wykorzystywane do zarządzania środowiskiem uruchomieniowym i ich wpływ na eksperymenty.

Proponowana zawartość:
	•	Docker:
	•	    Do czego jest wykorzystywany: uruchamianie lokalnych usług, środowiska testowe i deweloperskie, konteneryzacja narzędzi, konteneryzacja produkcyjnych aplikacji.
	•	    Jak mozna popelnic bledy
	•		Warto pokazać typowe błędy, które LLM może popełnić (np. użycie ADD zamiast COPY, brak WORKDIR, użycie latest tagu, brak usuwania zależności po buildzie). To od razu pokazuje, dlaczego ocena bezpieczeństwa i optymalizacji jest tak ważna.
	•	Kubernetes:
	•	    Używany z minimalnym zakresem (np. przez Kind, K3s, minikube).
	•	    Rola w testowaniu konfiguracji i IaC.
	•		Warto podkreślić, że Kubernetes służy jako środowisko do walidacji runtime wygenerowanych konfiguracji, a nie tylko do ich tworzenia.
	•	    Jak można popełnić błędy przy generowaniu YAML (niedopasowane zasoby, problemy z dostępnością, błędne konfiguracje).
	•		Dodaj przykłady: brak liveness/readiness probes, źle skonfigurowane serwisy (np. ClusterIP zamiast NodePort/LoadBalancer), brak PersistentVolumeClaim dla baz danych, zbyt wysokie/niskie limity zasobów, brak kontekstu bezpieczeństwa.
	•	Dobre i złe praktyki:
	•	    Przykłady z testów (np. źle ustawione limity zasobów, niepoprawne labele).
	•   Dodac tu Kaniko czy w technologiach wspierajacych?
	•	Chyba nie bedzie wykorzystywane jeszcze tutaj, a jedynie w systemie PaaS



\subsection{Narzędzia do oceny jakości IaC}

Cel: Pokazać narzędzia do analizy i walidacji konfiguracji IaC oraz ich funkcjonalności.

Proponowana zawartość:
	•	Lista narzędzi:
	•	    checkov, kube-linter, kube-score, kubeval, terrascan, cloudeval-yaml, iac-eval.
	•	Opis funkcji i zasad działania:
	•	    Statyczna analiza, reguły zgodności, walidacja schematów.
	•	Przykłady outputów:
	•	    Co zgłaszają narzędzia, jak to interpretować.
	•	    Zestawienie wyników na tych samych plikach (jeśli masz dane).
	•		Pokazać kilka błędów mniejszych i większych, podatności itd



\subsection{Środowisko eksperymentalne}

Cel: Przedstawić infrastrukturę testową i środowisko, w którym uruchamiane będą eksperymenty.

Proponowana zawartość:
	•	Charakterystyka środowiska:
	•	    Lokalna maszyna, Docker Desktop, Kubectl, Kind, Python, LangChain, LangGraph, OpenRouter?.
	•		Jasno określ, co jest na maszynie lokalnej, a co jest usługą zewnętrzną (OpenRouter to API gateway).
	•	Parametry techniczne:
	•	    RAM, CPU, ewentualna obecność GPU, ograniczenia związane z lokalnością - dlaczego lokalnie?
	•	Zasady testowania:
	•	    Jak wygląda testowanie przez API: limity żądań, obsługa błędów, wersje modeli.
	•	    Korzystanie z narzędzi CLI lub SDK.



\subsection{Technologie wspierające (do rozważenia jako część środowiska eksperymentalnego)}

Alternatywa 1: Zostawić jako osobny rozdział z wyraźnym celem: technologie pomocnicze ułatwiające pracę.

Alternatywa 2: Zmergować z poprzednim podrozdziałem jako „środowisko eksperymentalne i wspierające technologie”.

Zawartość:
	•	Język Python:
	•	    Używane biblioteki (np. requests, openai, langchain, pydantic, git, docker-py itd.).
	•	Narzędzia pomocnicze:
	•	    Kind, Lens, VSCode, Docker CLI, kubectl, LangSmith, LangFuse, Promptfoo, Kaniko.
	•		LangSmith, LangFuse, Promptfoo: To jest bardzo ważne! Są to narzędzia do ewaluacji i zarządzania promptami/LLM. Koniecznie je opisz krótko i wyjaśnij, jak pomogły w Twoich badaniach (np. do monitorowania interakcji z LLM, testowania promptów, porównywania ich skuteczności). To pokazuje zaawansowane podejście do inżynierii promptów.
	•	Rola:
	•	    Obsługa eksperymentów, monitorowanie, automatyzacja.


