\clearpage % Rozdziały zaczynamy od nowej strony.

\section{Przegląd technologii i narzędzi}

Celem tego rozdziału jest przedstawienie najważniejszych technologii i narzędzi, stanowiących podstawę niniejszego badania. Szczególny nacisk położony zostanie na duże modele językowe (LLM), opis ich architektur oraz ich zastosowanie w generowaniu konfiguracji Infrastructure as Code (IaC). Ponadto, omówione zostaną kluczowe narzędzia DevOps, takie jak Docker i Kubernetes, niezbędne w zarządzaniu środowiskami uruchomieniowymi, oraz narzędzia służące do walidacji i oceny jakości generowanego kodu IaC. Rozdział zawiera także opis środowiska eksperymentalnego oraz wykorzystywanych technologii wspierających automatyzację i monitorowanie eksperymentów.

\subsection{Modele językowe wykorzystywane w badaniu}

Duże modele językowe (LLM) pełnią istotną rolę w prezentowanych eksperymentach, służąc jako generatory konfiguracji IaC. Ich zdolność do przetwarzania języka naturalnego oraz generowania poprawnego kodu czyni je istotnym elementem automatyzacji procesów DevOps. W tym podrozdziale przedstawiono charakterystykę wybranych modeli, opis ich architektur oraz praktyczne aspekty związane z ich wykorzystaniem.

Rynek dużych modeli językowych rozwija się dynamicznie, oferując zarówno modele komercyjne, dostępne przez API, jak i modele open-source, możliwe do uruchomienia lokalnie lub na własnej infrastrukturze. Decyzja o wyborze konkretnego rodzaju modelu często zależy od dostępności zasobów sprzętowych (zwłaszcza GPU), wymagań dotyczących licencji, kwestii związanych z prywatnością danych oraz możliwości dostosowania modeli do specyficznych zastosowań.

Modele komercyjne dostępne przez API zazwyczaj cechują się wysoką wydajnością, zaawansowanymi możliwościami oraz niezawodnością, dzięki wsparciu dużych firm technologicznych takich jak OpenAI, Anthropic czy Google. Modele te trenowane są na szerokich i zróżnicowanych zbiorach danych, co przekłada się na ich zdolność generowania wysokiej jakości tekstu oraz kodu. Ich użytkowanie wiąże się z kosztami związanymi z opłatami za dostęp do API oraz koniecznością przesyłania danych do zewnętrznych serwisów, co może być problematyczne w przypadku danych wrażliwych.

Wybrane do badania modele komercyjne to:

\begin{itemize}
	\item \textbf{OpenAI GPT-4.1}, \textbf{GPT-4o} oraz \textbf{O3}: Najnowsze flagowe modele OpenAI. GPT-4.1 charakteryzuje się udoskonalonymi możliwościami generowania i rozumienia złożonego kodu oraz zoptymalizowanymi funkcjami integracji z narzędziami (tool calling). GPT-4o, model multimodalny, oferuje najwyższą wydajność, elastyczność oraz wszechstronne możliwości interakcji. O3 zapewnia optymalne połączenie wydajności, szybkości oraz efektywności kosztowej.
	\item \textbf{Anthropic Claude Opus 4}, \textbf{Sonnet 4}, oraz \textbf{Haiku 3.5}: Zaawansowane modele Anthropic, cenione za bezpieczeństwo, zgodność z najlepszymi praktykami AI Safety oraz obsługę długich kontekstów i precyzyjnych instrukcji. Opus 4 to najbardziej zaawansowana wersja, Sonnet 4 równoważy wydajność z szybkością, a Haiku 3.5 skupia się na efektywności kosztowej i szybkości działania.
	\item \textbf{Google Gemini 2.5 Pro} i \textbf{2.5 Flash}: Modele Google z rozszerzonym oknem kontekstu, dedykowane do analizy obszernych repozytoriów kodu i dokumentacji. Wersja Pro oferuje wyjątkową pojemność kontekstu i najwyższą wydajność, podczas gdy wersja Flash jest zoptymalizowana pod kątem szybszego działania przy niższych kosztach operacyjnych.
\end{itemize}

Modele open-source stanowią atrakcyjną alternatywę, oferując pełną kontrolę, możliwość uruchomienia na własnej infrastrukturze oraz brak dodatkowych opłat za każde zapytanie. Ich wydajność zależy przede wszystkim od dostępności zasobów sprzętowych, a aktywny rozwój społeczności przyczynia się do ciągłej optymalizacji i pojawiania się coraz bardziej zaawansowanych wersji.

Wybrane do badania modele open-source to:

\begin{itemize}
	\item \textbf{Mistral Medium} oraz \textbf{Codestral}: Modele od firmy Mistral AI, zoptymalizowane pod kątem efektywnego generowania kodu i ogólnego rozumienia kontekstu technicznego. Codestral specjalizuje się szczególnie w obsłudze złożonych zadań programistycznych, oferując wysoką efektywność przy umiarkowanych wymaganiach sprzętowych.
	\item \textbf{Meta Llama 4 Maverick} oraz \textbf{Llama 4 Scout}: Najnowsze modele open-source od Meta, które stanowią znaczący punkt odniesienia w dziedzinie rozwoju sztucznej inteligencji. Maverick wyróżnia się najwyższymi osiągami i zaawansowanymi funkcjami, natomiast Scout zapewnia dobrą wydajność przy bardziej ograniczonych zasobach sprzętowych, umożliwiając łatwiejsze wdrożenie w praktycznych zastosowaniach.
\end{itemize}

Wszystkie wymienione modele reprezentują aktualny szczyt osiągnięć w dziedzinie dużych modeli językowych (LLM). Wybrane zostały przede wszystkim ze względu na ich potwierdzone zdolności do generowania wysokiej jakości kodu oraz wsparcie dla funkcji „tool calling”, które umożliwiają bezpośrednią integrację modeli z zewnętrznymi narzędziami lub usługami poprzez dedykowane mechanizmy API bądź platformy integracyjne, takie jak LangChain czy LangGraph. Funkcje te będą odgrywać istotną rolę w proponowanym w tej pracy podejściu opartym na pętli sprzężenia zwrotnego w procesie generowania konfiguracji Infrastructure as Code (IaC).

TODO tu skonczylem

Cel: Przedstawić modele LLM używane w eksperymencie, ich charakterystyki, sposób użycia i ograniczenia.

Proponowana zawartość:
	•	Rodzaje modeli:
	•	    Podział na modele open-source (np. Mistral, LLaMA, Nous Hermes) vs. modele komercyjne dostępne przez API (np. GPT-4, Claude).
	•		Warto podkreślić, że wybór między open-source a komercyjnymi często zależy od dostępności zasobów (GPU), wymagań licencyjnych i elastyczności.
	•	Architektury i charakterystyki:
	•	    Typowe parametry: liczba parametrów, specjalizacje, wersje, okno kontekstu (dlaczego wazne przy repozytoriach kodu)
	•		Warto dodać, że "liczba parametrów" nie zawsze jest jedynym wyznacznikiem jakości, ale jest istotnym parametrem. "Okna kontekstu" – jak najbardziej warto wyjaśnić, dlaczego jest ważne przy repozytoriach kodu (np. umożliwia dostarczanie całych plików, fragmentów dokumentacji, wielu powiązanych promptów jednocześnie).
    •   Wybrane modele do badania:
    •       Krótki opis które i dlaczego z parametrami jakimiś basic
	• 		Informacja, które modele są używane w badaniu i dlaczego (ograniczenia przez tool calling w langgraph).
	•		To jest bardzo ważna informacja! Musisz to jasno przedstawić. Jeśli LangGraph narzuca ograniczenia na wybór modeli (np. tylko te z dobrze zaimplementowanym tool calling), to jest to istotny czynnik wpływający na zakres badań.
	•	Sposób użycia:
	•	    Poprzez OpenRouter, bezpośrednie API, LangChain / LangGraph.
	•	    Sposób integracji z agentami.
	•	Tryby promptowania:
	•	    Zero-shot, few-shot, chain-of-thought, agent-based.
	•	    Porównanie tych podejść i wybór odpowiednich metod do problemu.
	•		Warto wyjaśnić, dlaczego wybrane metody (np. agent-based) są najbardziej odpowiednie dla generowania konfiguracji IaC.
	•	Ograniczenia i praktyczne aspekty:
	•	    Licencje, limity API, ograniczona liczba modeli.
	•	    Context window – jak wpływa na eksperymenty z kodem.
	•	    Brak fine-tuningu – tylko inference.
	•	    Kontrola nad danymi – brak możliwości dostosowania kodu źródłowego w zamkniętych modelach.



\subsection{Narzędzia DevOps: Docker i Kubernetes}

Cel: Opisać narzędzia wykorzystywane do zarządzania środowiskiem uruchomieniowym i ich wpływ na eksperymenty.

Proponowana zawartość:
	•	Docker:
	•	    Do czego jest wykorzystywany: uruchamianie lokalnych usług, środowiska testowe i deweloperskie, konteneryzacja narzędzi, konteneryzacja produkcyjnych aplikacji.
	•	    Jak mozna popelnic bledy
	•		Warto pokazać typowe błędy, które LLM może popełnić (np. użycie ADD zamiast COPY, brak WORKDIR, użycie latest tagu, brak usuwania zależności po buildzie). To od razu pokazuje, dlaczego ocena bezpieczeństwa i optymalizacji jest tak ważna.
	•	Kubernetes:
	•	    Używany z minimalnym zakresem (np. przez Kind, K3s, minikube).
	•	    Rola w testowaniu konfiguracji i IaC.
	•		Warto podkreślić, że Kubernetes służy jako środowisko do walidacji runtime wygenerowanych konfiguracji, a nie tylko do ich tworzenia.
	•	    Jak można popełnić błędy przy generowaniu YAML (niedopasowane zasoby, problemy z dostępnością, błędne konfiguracje).
	•		Dodaj przykłady: brak liveness/readiness probes, źle skonfigurowane serwisy (np. ClusterIP zamiast NodePort/LoadBalancer), brak PersistentVolumeClaim dla baz danych, zbyt wysokie/niskie limity zasobów, brak kontekstu bezpieczeństwa.
	•	Dobre i złe praktyki:
	•	    Przykłady z testów (np. źle ustawione limity zasobów, niepoprawne labele).
	•   Dodac tu Kaniko czy w technologiach wspierajacych?
	•	Chyba nie bedzie wykorzystywane jeszcze tutaj, a jedynie w systemie PaaS



\subsection{Narzędzia do oceny jakości IaC}

Cel: Pokazać narzędzia do analizy i walidacji konfiguracji IaC oraz ich funkcjonalności.

Proponowana zawartość:
	•	Lista narzędzi:
	•	    checkov, kube-linter, kube-score, kubeval, terrascan, cloudeval-yaml, iac-eval.
	•	Opis funkcji i zasad działania:
	•	    Statyczna analiza, reguły zgodności, walidacja schematów.
	•	Przykłady outputów:
	•	    Co zgłaszają narzędzia, jak to interpretować.
	•	    Zestawienie wyników na tych samych plikach (jeśli masz dane).
	•		Pokazać kilka błędów mniejszych i większych, podatności itd



\subsection{Środowisko eksperymentalne}

Cel: Przedstawić infrastrukturę testową i środowisko, w którym uruchamiane będą eksperymenty.

Proponowana zawartość:
	•	Charakterystyka środowiska:
	•	    Lokalna maszyna, Docker Desktop, Kubectl, Kind, Python, LangChain, LangGraph, OpenRouter?.
	•		Jasno określ, co jest na maszynie lokalnej, a co jest usługą zewnętrzną (OpenRouter to API gateway).
	•	Parametry techniczne:
	•	    RAM, CPU, ewentualna obecność GPU, ograniczenia związane z lokalnością - dlaczego lokalnie?
	•	Zasady testowania:
	•	    Jak wygląda testowanie przez API: limity żądań, obsługa błędów, wersje modeli.
	•	    Korzystanie z narzędzi CLI lub SDK.



\subsection{Technologie wspierające (do rozważenia jako część środowiska eksperymentalnego)}

Alternatywa 1: Zostawić jako osobny rozdział z wyraźnym celem: technologie pomocnicze ułatwiające pracę.

Alternatywa 2: Zmergować z poprzednim podrozdziałem jako „środowisko eksperymentalne i wspierające technologie”.

Zawartość:
	•	Język Python:
	•	    Używane biblioteki (np. requests, openai, langchain, pydantic, git, docker-py itd.).
	•	Narzędzia pomocnicze:
	•	    Kind, Lens, VSCode, Docker CLI, kubectl, LangSmith, LangFuse, Promptfoo, Kaniko.
	•		LangSmith, LangFuse, Promptfoo: To jest bardzo ważne! Są to narzędzia do ewaluacji i zarządzania promptami/LLM. Koniecznie je opisz krótko i wyjaśnij, jak pomogły w Twoich badaniach (np. do monitorowania interakcji z LLM, testowania promptów, porównywania ich skuteczności). To pokazuje zaawansowane podejście do inżynierii promptów.
	•	Rola:
	•	    Obsługa eksperymentów, monitorowanie, automatyzacja.


