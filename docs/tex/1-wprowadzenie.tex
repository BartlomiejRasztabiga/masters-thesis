\clearpage % Rozdziały zaczynamy od nowej strony.
\section{Wprowadzenie}

W ciągu ostatnich lat obserwujemy bezprecedensowy rozwój dużych modeli językowych (LLM) oraz równoległy wzrost znaczenia technologii konteneryzacji i orkiestracji. Modele językowe takie jak GPT, LLaMA, Falcon czy Claude wykazują zdolność do generowania złożonego kodu, w tym konfiguracji infrastruktury, podczas gdy Docker i Kubernetes ugruntowały swoją pozycję jako standard w obszarze wdrażania aplikacji. Niniejsza praca bada potencjał automatyzacji generowania konfiguracji Docker i Kubernetes z wykorzystaniem LLM, ze szczególnym uwzględnieniem aspektów poprawności i bezpieczeństwa generowanych plików konfiguracyjnych.

\subsection{Cel i zakres pracy}

Celem niniejszej pracy magisterskiej jest analiza możliwości zastosowania dużych modeli językowych (LLM) do automatycznego generowania konfiguracji typu Infrastructure as Code (IaC), ze szczególnym uwzględnieniem plików Dockerfile oraz manifestów Kubernetes. Praca skupia się na wykorzystaniu LLM w kontekście platformy jako usługi (PaaS), gdzie efektywne tworzenie i utrzymanie kontenerów oraz ich orkiestracji ma kluczowe znaczenie. Zakres obejmuje zbadanie metod generowania plików Dockerfile definiujących obrazy kontenerów oraz manifestów Kubernetes opisujących wdrożenie tych kontenerów, a także oceny jakości, bezpieczeństwa i zgodności takich wygenerowanych konfiguracji z wymaganiami systemowymi.

Głównym celem pracy jest analiza możliwości zastosowania dużych modeli językowych do generowania konfiguracji Dockerfile i Kubernetes w ramach platformy jako usługi (PaaS), która umożliwia budowanie i wdrażanie aplikacji w klastrze Kubernetes. Praca obejmuje następujące zagadnienia:

\subsection{Motywacja}

Tematyka pracy jest istotna z uwagi na rosnącą rolę metodyk DevOps i automatyzacji zarządzania infrastrukturą. Infrastructure as Code zyskuje na popularności, ponieważ umożliwia spójną, powtarzalną konfigurację środowisk i redukcję błędów ludzkich. Ręczne tworzenie skryptów IaC, zwłaszcza dla złożonych środowisk chmurowych, bywa jednak czasochłonne i wymaga specjalistycznej wiedzy. Wraz z rozwojem dużych modeli językowych pojawiła się możliwość ich wykorzystania do generowania kodu konfiguracyjnego na podstawie opisu w języku naturalnym. Użycie LLM może obniżyć barierę wejścia dla deweloperów mniej doświadczonych w technologiach chmurowych (np. Kubernetes) poprzez automatyczne tłumaczenie wysokopoziomowych specyfikacji na deklaratywne pliki konfiguracji. Ma to znaczenie praktyczne na platformach PaaS, gdzie skrócenie czasu wdrożenia aplikacji i eliminacja błędów konfiguracji przekładają się na większą wydajność i niezawodność usług. Z drugiej strony, automatyzacja generowania IaC rodzi pytania o poprawność i bezpieczeństwo tych konfiguracji. Modele językowe mogą popełniać błędy lub tzw. halucynacje, generując nieistniejące lub niezalecane elementy konfiguracji. Istotne jest zatem zbadanie, na ile można zaufać LLM w kontekście krytycznych elementów infrastruktury oraz jak zapewnić zgodność wygenerowanych konfiguracji z najlepszymi praktykami bezpieczeństwa (np. czy model nie pominie istotnych zabezpieczeń, jak autoryzacja, czy nie wygeneruje podatnych ustawień). Zainteresowanie połączeniem LLM i DevOps wynika także z potencjału ułatwienia pracy inżynierów – dzięki LLM mogą oni szybciej uzyskiwać wstępne wersje konfiguracji i skupić się na ich dostrojeniu, zamiast pisać je od podstaw.

\subsection{Struktura pracy}

Praca składa się z dziewięciu rozdziałów, które wspólnie odzwierciedlają pełny cykl badawczy — od identyfikacji problemu, przez analizę literatury i technologii, aż po eksperymenty, analizę wyników, bezpieczeństwo i implementację kompletnego systemu.

Rozdział 1 – Wprowadzenie: przedstawia temat pracy, jej cele, motywację oraz układ całej pracy.
Rozdział 2 – Przegląd literatury: zawiera analizę aktualnych badań nad wykorzystaniem dużych modeli językowych w kontekście generowania Infrastructure as Code (IaC), w tym konfiguracji Docker i Kubernetes. Wskazuje także istniejące luki, np. brak analiz bezpieczeństwa i brak pełnych pipeline’ów DevOps.
Rozdział 3 – Przegląd technologii i narzędzi: opisuje wykorzystane modele językowe (GPT, Claude, LLaMA, Mistral, DeepSeek) oraz narzędzia infrastrukturalne (Docker, Kubernetes, Lens, kind). Rozdział porównuje też podejścia API vs open-source, opisuje środowisko uruchomieniowe i technologie wspierające (FastAPI, GitPython itp.).
Rozdział 4 – Eksperymenty: opisuje zaplanowane przypadki testowe (np. aplikacje jedno- i wielosystemowe), strategie promptowania, proces generowania i testowania konfiguracji oraz przykładowe wyniki i napotkane problemy (np. ograniczenia tokenów).
Rozdział 5 – Analiza porównawcza modeli LLM: przedstawia kryteria oceny (poprawność, bezpieczeństwo, deterministyczność, odporność na manipulacje, wydajność), wyniki testów i wnioski na temat jakości działania poszczególnych modeli.
Rozdział 6 – Bezpieczeństwo konfiguracji: analizuje zagrożenia związane z automatycznym generowaniem konfiguracji, techniki ataku na LLM (prompt injection, jailbreaking) oraz metody oceny i wzmacniania bezpieczeństwa wygenerowanego kodu.
Rozdział 7 – Projekt systemu PaaS: prezentuje architekturę zaprojektowanego systemu do automatycznego generowania, budowania i wdrażania aplikacji. Opisuje także problemy projektowe oraz proces działania od repozytorium do uruchomienia aplikacji w Kubernetes.
Rozdział 8 – Implementacja i wdrożenie prototypu: zawiera szczegółowy opis realizacji systemu PaaS, użytych technologii, przykładowych wdrożeń oraz napotkanych problemów podczas integracji i testów.
Rozdział 9 – Wnioski i dalsze kierunki rozwoju: podsumowuje uzyskane wyniki, odpowiada na pytania badawcze, ocenia przydatność LLM w środowiskach DevOps oraz wskazuje możliwe ścieżki rozwoju — takie jak wsparcie mikroserwisów, walidacja semantyczna YAML czy integracja z CI/CD.