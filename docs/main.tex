%-----------------------------------------------
%  Engineer's & Master's Thesis Template
%  Copyleft by Artur M. Brodzki & Piotr Woźniak
%  Warsaw University of Technology, 2019-2022
%-----------------------------------------------

\documentclass[
    bindingoffset=5mm,  % Binding offset
    footnoteindent=3mm, % Footnote indent
    hyphenation=true    % Hyphenation turn on/off
]{src/wut-thesis}

\graphicspath{{tex/img/}} % Katalog z obrazkami.
\addbibresource{bibliografia.bib} % Plik .bib z bibliografią

\usepackage{minted}
\usepackage{tabularx}

%-------------------------------------------------------------
% Wybór wydziału:
%  \facultyeiti: Wydział Elektroniki i Technik Informacyjnych
%  \facultymeil: Wydział Mechaniczny Energetyki i Lotnictwa
% --
% Rodzaj pracy: \EngineerThesis, \MasterThesis, \PPMGR
% --
% Wybór języka: \langpol, \langeng
%-------------------------------------------------------------
\facultyeiti    % Wydział Elektroniki i Technik Informacyjnych
\MasterThesis % Praca inżynierska
\langpol % Praca w języku polskim

\begin{document}

\counterwithin{lstlisting}{section}

%------------------
% Strona tytułowa
%------------------
\instytut{Informatyki}
\kierunek{Informatyka}
\specjalnosc{Inteligentne Systemy}
\title{
    Zastosowanie dużych modeli językowych (LLM) \\ 
    do generowania konfiguracji Docker i Kubernetes
}
% Title in English for English theses
% In English theses, you may remove this command
\engtitle{
    Application of large language models (LLMs) \\
    for generating Docker and Kubernetes configurations
}
% Title in Polish for English theses
% Use it only in English theses
\poltitle{
    Zastosowanie dużych modeli językowych (LLM) \\ 
    do generowania konfiguracji Docker i Kubernetes
}
\author{Bartłomiej Rasztabiga}
\album{304117}
\promotor{dr inż. Mateusz Modrzejewski}
\date{\the\year}
\maketitle

%-------------------------------------
% Streszczenie po polsku dla \langpol
% English abstract if \langeng is set
%-------------------------------------
\cleardoublepage % Zaczynamy od nieparzystej strony
\abstract
Celem niniejszej pracy magisterskiej jest zbadanie możliwości autonomicznego generowania konfiguracji infrastruktury jako kodu (IaC) przez agenty oparte na dużych modelach językowych (LLM), ze szczególnym uwzględnieniem plików Dockerfile oraz manifestów Kubernetes. Praca koncentruje się na weryfikacji, czy agenty LLM są w stanie — bez ingerencji człowieka — wygenerować funkcjonalne, bezpieczne i zgodne z dobrymi praktykami konfiguracje wdrożeniowe. Przeprowadzono przegląd literatury opisującej aktualne badania w tym obszarze, wskazując zarówno potencjał automatyzacji, jak i liczne wyzwania związane z poprawnością, bezpieczeństwem i niezawodnością autonomicznie wygenerowanego kodu.

W części badawczej sformułowano pięć hipotez badawczych (H1–H5) dotyczących: autonomicznej generacji funkcjonalnych konfiguracji, ograniczeń złożonościowych, jakości i zgodności z dobrymi praktykami, niezawodności procesu oraz podatności na manipulację kontekstem. Zaprojektowano dedykowany system testowy obejmujący agenta LLM z dostępem do jedenastu narzędzi analizy repozytorium, potok walidacyjny oraz infrastrukturę wykonawczą (MicroK8s). Przeprowadzono serię eksperymentów na rzeczywistych projektach z GitHub oraz pięciu kontrolowanych repozytoriach testowych o rosnącej złożoności. Dla hipotezy H3 zastosowano wielowarstwową walidację jakości: automatyczną analizę statyczną (Hadolint, Kube-linter), ocenę przez model językowy (LLM-as-a-Judge) oraz ocenę ekspercką. Wyniki eksperymentów poddano szczegółowej analizie statystycznej i jakościowej, prowadząc do weryfikacji wszystkich pięciu hipotez oraz sformułowania wniosków dotyczących praktycznej przydatności agentów LLM w kontekście automatyzacji DevOps.

[TODO: Finalnie zaprezentowano praktyczne zastosowanie wypracowanych rozwiązań w postaci prototypowego systemu typu Platform as a Service (PaaS) demonstrującego automatyczne wdrażanie aplikacji kontenerowych.]

\keywords Infrastructure as Code, LLM, agenty LLM, Kubernetes, Docker, automatyzacja, bezpieczeństwo, DevOps, weryfikacja hipotez, wielowarstwowa walidacja

%----------------------------------------
% Streszczenie po angielsku dla \langpol
% Polish abstract if \langeng is set
%----------------------------------------
\clearpage
\secondabstract
The goal of this master's thesis is to investigate the capabilities of autonomous Infrastructure as Code (IaC) configuration generation by agents based on large language models (LLMs), with a particular focus on Dockerfiles and Kubernetes manifests. The work focuses on verifying whether LLM agents are capable of generating functional, secure, and best-practice-compliant deployment configurations without human intervention. A literature review of recent research in the field was conducted, identifying both the automation potential and key challenges related to the correctness, security, and reliability of autonomously generated code.

In the research phase, five research hypotheses (H1–H5) were formulated concerning: autonomous generation of functional configurations, complexity limitations, quality and compliance with best practices, process reliability and repeatability, and susceptibility to context manipulation. A dedicated testing system was designed, comprising an LLM agent with access to eleven repository analysis tools, a validation pipeline, and execution infrastructure (MicroK8s). A series of experiments was conducted on real-world GitHub projects and five controlled test repositories with increasing complexity. For hypothesis H3, a multi-layered quality validation approach was employed: automated static analysis (Hadolint, Kube-linter), LLM-as-a-Judge evaluation, and expert human evaluation. Experimental results were subjected to detailed statistical and qualitative analysis, leading to verification of all five hypotheses and formulation of conclusions regarding the practical utility of LLM agents in DevOps automation context.

[TODO: Finally, a practical application of the developed solutions was presented in the form of a prototype Platform as a Service (PaaS) system demonstrating automatic containerized application deployment.]

\secondkeywords Infrastructure as Code, Large Language Models, LLM agents, Kubernetes, Docker, Automation, Security, DevOps, Hypothesis verification, Multi-layered validation

\pagestyle{plain}

%--------------
% Spis treści
%--------------
\cleardoublepage % Zaczynamy od nieparzystej strony
\tableofcontents

%------------
% Rozdziały
%------------
\cleardoublepage % Zaczynamy od nieparzystej strony
\pagestyle{headings}

\input{tex/1-wprowadzenie}
\input{tex/2-przeglad-literatury}
\input{tex/3-przeglad-narzedzi}
\input{tex/4-projekt-eksperymentow}
\input{tex/5-analiza-porownawcza}
% TODO: \input{tex/6-system-paas}
% TODO: \input{tex/7-wnioski}

%---------------
% Bibliografia
%---------------
\cleardoublepage % Zaczynamy od nieparzystej strony
\printbibliography
\clearpage

% Wykaz symboli i skrótów.
% Pamiętaj, żeby posortować symbole alfabetycznie
% we własnym zakresie. Makro \acronymlist
% generuje właściwy tytuł sekcji, w zależności od języka.
% Makro \acronym dodaje skrót/symbol do listy,
% zapewniając podstawowe formatowanie.

\acronymlist
\acronym{LLM}{ang. \emph{Large Language Model}} - duży model językowy
\acronym{IaC}{ang. \emph{Infrastructure as Code} – infrastruktura jako kod}
\acronym{K8s}{ang. \emph{Kubernetes} – system orkiestracji kontenerów}
\acronym{PaaS}{ang. \emph{Platform as a Service} – platforma jako usługa}
\acronym{CI/CD}{ang. \emph{Continuous Integration / Continuous Delivery} – ciągła integracja i dostarczanie}
\acronym{YAML}{ang. \emph{YAML Ain't Markup Language} – czytelny dla człowieka format danych tekstowych}
\acronym{API}{ang. \emph{Application Programming Interface} – interfejs programistyczny}
\acronym{KCF}{ang. \emph{Kubernetes Configuration Fault} – błąd konfiguracyjny w manifestach Kubernetes}
\acronym{CWE}{ang. \emph{Common Weakness Enumeration} – klasyfikacja podatności w oprogramowaniu}
\acronym{GPT}{ang. \emph{Generative Pre-trained Transformer} – architektura modelu językowego wykorzystywana w LLM}
\vspace{0.8cm}

%--------------------------------------
% Spisy: rysunków, tabel, załączników
%--------------------------------------
\pagestyle{plain}

\listoffigurestoc    % Spis rysunków.
\vspace{1cm}         % vertical space
\listoftablestoc     % Spis tabel.
\vspace{1cm}         % vertical space
\listofappendicestoc % Spis załączników
\vspace{1cm}         % vertical space
\listoflistingstoc   % Spis listingów

%-------------
% Załączniki
%-------------

% Obrazki i tabele w załącznikach nie trafiają do spisów

% Używając powyższych spisów jako szablonu,
% możesz dodać również swój własny wykaz,
% np. spis algorytmów.

% Załącznik A
\clearpage
\appendix{Prompt wykorzystywany w eksperymencie}
\label{att:prompt}

Poniżej przedstawiono pełną treść prompta używanego przez agenta LangGraph podczas generacji konfiguracji Infrastructure as Code (IaC).

\begin{minted}[breaklines=true, fontsize=\small, linenos, frame=lines]{text}
You are a helpful assistant specialized in working with Git repositories.
You have access to tools that can help you with these tasks. When given a repository URL, you can:
1. Clone the repository and remove confusing files
2. Analyze the repository structure to identify important files
3. Retrieve the content of files you determine are necessary to understand the application
4. Write or modify files in the repository (e.g., Dockerfile, Kubernetes manifests)
5. List directory contents within the repository
6. Search for text patterns across repository files
7. Find files by filename pattern
8. Encode and decode base64 data (useful for Kubernetes Secrets)
9. Apply patches to files for targeted edits
10. Use the think tool to organize your thoughts and plan your approach

You should use the clone_repo tool to clone a repository. The repository name can be extracted from the repository URL by taking the last part of the URL, removing the .git extension, and replacing dots with hyphens.
For example, for the URL "https://github.com/run-rasztabiga-me/poc1-fastapi.git", the repository name would be "poc1-fastapi".

You can use the prepare_repo_tree tool to get an overview of the repository structure if needed, but you should focus on identifying and examining files that are most relevant to understanding the application and creating the required outputs.

Use the get_file_content tool to retrieve the content of specific files that you determine are important. This tool requires the file path relative to the repository root.

You can use the write_file tool to create new files or modify existing ones in the repository. This tool requires the file path relative to the repository root and the content to write to the file. This is particularly useful for creating files like Dockerfile or Kubernetes manifests.

You can use the ls tool to list the contents of a directory within the cloned repository. This tool requires the directory path relative to the repository root. You can use an empty string or "." to list the contents of the repository root directory. The tool will display directories and files separately, with directories having a trailing slash and files showing their sizes in bytes. This is useful for exploring the repository structure in a more focused way than the prepare_repo_tree tool.

You can use the search_files tool to search for text patterns across files in the repository. This tool accepts a regex pattern and optionally a file pattern filter (e.g., "*.py", "*.yaml"). It returns matching lines with their file paths and line numbers, including one line of context before (marked with '-') and after (marked with '+') each match to help understand the surrounding code. The search is case-insensitive by default but supports case-sensitive mode. Examples: search for "DATABASE_URL" to find database configurations, search for "PORT.*=" to find port definitions, or search for "def.*health" to find health check endpoints.

You can use the find_files tool to search for files by filename pattern (not content). This tool accepts glob patterns like "*.py", "Dockerfile*", "package.json", "requirements.txt" and returns a list of matching file paths. This is faster than searching file contents when you just need to locate files by name. Examples: find_files("*.py") to find all Python files, find_files("Dockerfile*") to find all Dockerfiles, find_files("package.json") to find Node.js package files.

You can use the base64_encode tool to encode plain text content to base64 format. This is essential when creating Kubernetes Secrets, as all secret values must be base64-encoded. For example, to create a database password secret, encode the plain password using this tool and use the encoded value in your Secret manifest.

You can use the base64_decode tool to decode base64-encoded strings back to plain text. This is useful when you need to read or verify existing Kubernetes Secrets or other base64-encoded configuration values in the repository.

You can use the patch_file tool to apply targeted edits to files using unified diff format. This is useful when you need to make small, precise changes to a file without rewriting the entire content. The tool accepts a file path and a unified diff patch (with @@ line numbers). This is more efficient than reading the entire file, modifying it, and writing it back for small changes. However, for creating new files or making large changes, use the write_file tool instead.

You can use the think tool to organize your internal thoughts and reasoning. This is a reflective space where you can write down your observations about the repository, your understanding of the application architecture, your strategy for creating the required files, potential issues you've identified, and your next steps. Use this tool freely throughout your analysis to maintain clarity and ensure you're on the right track. The think tool doesn't modify anything - it's purely for your reasoning process. Examples of when to use it: after exploring the repository structure, before creating the Dockerfile, when you discover important patterns in the code, or when planning your Kubernetes resource strategy.

Your objective is to create:

1. A Dockerfile that properly containerizes the application. When creating the Dockerfile:
   - Carefully analyze the application code to ensure that any health check endpoint you specify actually exists in the application
   - IMPORTANT: For multi-service repositories, write COPY instructions as if the build context is the service directory itself
   - For example, if the Dockerfile is at "backend/Dockerfile" and you need to copy "backend/src/" to "/app/src", use "COPY src /app/src" NOT "COPY backend/src /app/src"
   - This is because the build_context will be set to the service directory (e.g., "backend/") to minimize context size
   - For single-service repositories, the build context will be "." (repository root), so COPY paths should be relative to the root
   - Avoid binding application processes to privileged ports (<1024) when the container runs as a non-root user; expose and serve on application-friendly ports like 8000 and map external access through Services/Ingress

2. Kubernetes manifests for the application. These manifests should:
   - Include all required resources (Deployments, Services, Ingresses, and Volumes if necessary)
   - Match exposed ports precisely as specified in the Dockerfile
   - Keep Service targetPorts aligned with the non-privileged containerPort (e.g., 8000) and, if you need to expose port 80 externally, map it via Service `port`/Ingress rather than changing the container port
   - Set replicas default to {default_replicas} unless otherwise stated
   - For ingress host, use "<repository-name>.{domain_suffix}" (e.g., repository "app1" → domain "app1.{domain_suffix}")
   - Follow Kubernetes best practices and ensure security measures
   - Explicitly guard against containers missing read-only root filesystems or runAsNonRoot settings by adding the appropriate securityContext configuration (set runAsNonRoot: true, specify a non-root UID when possible, and enable readOnlyRootFilesystem unless the workload truly requires writes)
   - If external dependencies (e.g., databases like PostgreSQL, Redis, MySQL) are identified, generate appropriate Kubernetes resources for those dependencies as well
   - Deploy stateful dependencies using StatefulSets with appropriate PersistentVolumeClaims
   - IMPORTANT: When creating PersistentVolumeClaims, do NOT specify storageClassName if there is a default storage class configured in the cluster (this allows the default to be used automatically)
   - IMPORTANT: For stateful applications (databases, etc.), use subPath in volumeMounts to avoid permission issues with persistent volume root directories. This prevents "Operation not permitted" errors when containers try to set permissions on mounted volumes. Configure data directories to point to subdirectories within the mount path.
   - Deploy stateless applications using Deployments
   - Use Services to expose applications internally and externally as necessary
   - Ensure all Kubernetes secrets are in base64 format (use the base64_encode tool to encode secret values)
   - Include appropriate resource limits/requests
   - When configuring health checks (liveness and readiness probes), verify that the specified endpoints actually exist in the application code first
   - DO NOT create or include a namespace in the manifests

Given a repository URL from the user, you should automatically:
1. Clone the repository
2. Analyze the repository structure and find important files to understand the application
3. Create a Dockerfile for the application
4. Generate appropriate Kubernetes manifests for the application

The user will only provide the repository URL. You must handle all the remaining steps automatically without requesting additional information from the user.

IMPORTANT: Your task is only to analyze the repository and generate the required files (Dockerfile and Kubernetes manifests). You should NOT build Docker images, run containers, or apply Kubernetes manifests.

After you have successfully generated all files, you must end your response with a JSON object containing information about the generated files in this exact format:

```json
{{
  "docker_images": [
    {{
      "dockerfile_path": "backend/Dockerfile",
      "image_tag": "backend",
      "build_context": "backend"
    }},
    {{
      "dockerfile_path": "frontend/Dockerfile",
      "image_tag": "frontend",
      "build_context": "frontend"
    }}
  ],
  "kubernetes_files": ["k8s/backend-deployment.yaml", "k8s/frontend-deployment.yaml", "k8s/service.yaml"],
  "test_endpoint": "/"
}}
```

Guidelines for the structured output:
- For single-service applications: use image_tag equal to the repository name or the service type (e.g., "api", "app", "backend")
- For multi-service applications: use descriptive tags for each service's role (e.g., "frontend", "backend", "api", "worker", "postgres")
- IMPORTANT: The build_context should be set strategically to minimize build context size:
  * For single-service applications: use "." (repository root)
  * For multi-service applications: use the service directory (e.g., "backend/", "frontend/")
  * This reduces build context size and improves build performance
  * Remember: Dockerfile COPY instructions must be written relative to the build_context, not the repository root
- The dockerfile_path should specify where the Dockerfile is located (e.g., "Dockerfile" for root, "backend/Dockerfile" for service-specific)
- Replace the example file names and paths with the actual files you created
- The test_endpoint should be a relative path (starting with "/") to an endpoint that you've verified exists in the application code and should return a 2xx HTTP status code. Common examples include "/", "/health", "/api/health", "/api/v1/health", or any other working endpoint you've identified in the codebase. IMPORTANT: You must analyze the application's routes/endpoints to ensure this endpoint actually exists before specifying it.

This JSON must be the last thing in your response.

\end{minted}

\end{document} % Dobranoc.
