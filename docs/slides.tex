\documentclass[aspectratio=169]{beamer}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage[polish]{babel}

\title{Zastosowanie dużych modeli językowych (LLM) do generowania konfiguracji Docker i Kubernetes}
\subtitle{Seminarium dyplomowe}
\author{Bartłomiej Rasztabiga}
\institute{Politechnika Warszawska, Wydział EiTI}
\date{\the\year}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}{Agenda}
    \begin{itemize}
        \item Motywacja i cel pracy
        \item Metodyka i system testowy
        \item Wyniki H1--H2
        \item Wnioski i dalsze prace
    \end{itemize}
\end{frame}

\begin{frame}{Problem i motywacja}
    \begin{itemize}
        \item IaC przyspiesza DevOps, ale wymaga specjalistycznej wiedzy
        \item Ręczne tworzenie Docker/K8s jest czasochłonne i podatne na błędy
        \item LLM mogą automatyzować generację, ale trzeba zbadać skuteczność i ryzyka
    \end{itemize}
\end{frame}

\begin{frame}{Cel pracy i hipotezy}
    \begin{itemize}
        \item Cel: ocena autonomicznej generacji konfiguracji IaC przez agenty LLM
        \item H1: działające konfiguracje end-to-end
        \item H2: spadek skuteczności wraz ze złożonością
        \item H3--H5: jakość, powtarzalność, podatność na manipulacje
    \end{itemize}
\end{frame}

\begin{frame}{Stan wiedzy}
    \begin{itemize}
        \item LLM w generowaniu Docker/K8s i IaC
        \item Podejścia typu ``Don't Train, Just Prompt''
        \item LLM w naprawie błędów IaC i w bezpieczeństwie
        \item Wyzwania: halucynacje, zgodność, bezpieczeństwo
    \end{itemize}
\end{frame}

\begin{frame}{Luka badawcza}
    \begin{itemize}
        \item Brak kompleksowej oceny end-to-end (build $\rightarrow$ apply $\rightarrow$ runtime)
        \item Niewystarczające dane o złożoności i powtarzalności
        \item Ograniczone analizy podatności na prompt injection
    \end{itemize}
\end{frame}

\begin{frame}{Koncepcja rozwiązania}
    \begin{itemize}
        \item Agent LLM analizuje repozytorium i generuje Dockerfile + manifesty
        \item Automatyczny pipeline walidacji
        \item Uruchomienie na klastrze MicroK8s
    \end{itemize}
\end{frame}

\begin{frame}{Architektura systemu testowego}
    \begin{itemize}
        \item Agent: LangGraph + narzedzia analizy repozytorium
        \item Walidacja: build, lint, dry-run, apply, test endpoint
        \item Infrastruktura: Docker + MicroK8s + lokalny registry
    \end{itemize}
\end{frame}

\begin{frame}{Narzędzia agenta i prompt}
    \begin{itemize}
        \item 11 narzędzi (clone, tree, search, read, write, patch)
        \item Zasady generacji (imagePullPolicy, probes, requests/limits)
        \item Wyjście: JSON z listą obrazów i manifestów
    \end{itemize}
\end{frame}

\begin{frame}{Metodyka i metryki}
    \begin{itemize}
        \item Metryki: build\_success, apply\_success, runtime\_success
        \item Wskaźniki warunkowe (apply$|$build, runtime$|$apply)
        \item Przedziały ufności Wilsona
    \end{itemize}
\end{frame}

\begin{frame}{Zbiory danych}
    \begin{itemize}
        \item H1: 25 publicznych repozytoriów webowych z GitHuba
        \item H2--H5: POC1--POC5 o rosnącej złożoności
        \item POC jailbreak dla H5
    \end{itemize}
\end{frame}

\begin{frame}{Wyniki H1 -- skuteczność}
    \begin{itemize}
        \item 94/150 sukcesów end-to-end = 62,7\%
        \item Build 78,7\%, Apply 77,7\%, Runtime 62,7\%
        \item Największy spadek na etapie runtime
    \end{itemize}
\end{frame}

\begin{frame}{Wyniki H1 -- porównanie modeli}
    \begin{itemize}
        \item GPT-5 Mini: 68,0\% sukcesu
        \item Gemini 2.5 Flash: 62,0\%
        \item DeepSeek-Chat: 58,0\%
        \item Różnice umiarkowane, brak dominacji jednego modelu
    \end{itemize}
\end{frame}

\begin{frame}{Wyniki H2 -- złożoność}
    \begin{itemize}
        \item Spadek skuteczności od POC1 do POC5 (100\% $\rightarrow$ 0\%)
        \item Trend silnie ujemny (Spearman $\approx$ -1,0)
        \item Problemy narastają przy multi-service i zależnościach
    \end{itemize}
\end{frame}

\begin{frame}{Typowe przyczyny awarii}
    \begin{itemize}
        \item Błędy build (npm ci, brak lockfile, złe COPY)
        \item Konflikty user/root i securityContext
        \item Niepoprawne endpointy zdrowia
        \item Problemy zależności i wersji runtime
    \end{itemize}
\end{frame}

\begin{frame}{H3--H5 -- status i obserwacje}
    \begin{itemize}
        \item H3: oczekiwane wysokie ostrzeżenia bez promptu best-practices
        \item H4: zmienność mimo temperature=0 (do analizy)
        \item H5: ryzyko prompt injection w repozytoriach
        \item Plan: uzupełnienie metryk i przykładów
    \end{itemize}
\end{frame}

\begin{frame}{Wnioski główne}
    \begin{itemize}
        \item Agenty LLM potrafią generować działające IaC w prostych repo
        \item Skuteczność spada gwałtownie przy złożoności
        \item Walidacja i ograniczenie kontekstu są kluczowe
    \end{itemize}
\end{frame}

\begin{frame}{Ograniczenia i dalsze prace}
    \begin{itemize}
        \item Ograniczona liczba repo i brak pełnych H3--H5
        \item Potrzeba rozszerzenia datasetu i automatycznej analizy jakości
        \item Demonstrator PaaS jako praktyczne zastosowanie wyników
    \end{itemize}
\end{frame}

\end{document}
